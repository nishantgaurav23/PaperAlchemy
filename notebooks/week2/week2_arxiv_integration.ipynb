{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: arXiv API Integration & PDF Processing\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 2 focuses on implementing the core data ingestion pipeline that will automatically fetch, process, and store arXiv papers. This is the foundation that feeds our RAG system with fresh academic content.\n",
    "\n",
    "## Week 2 Focus Areas\n",
    "\n",
    "### ðŸŽ¯ Core Objectives\n",
    "- **arXiv API Integration**: Build a robust client with rate limiting and retry logic\n",
    "- **PDF Processing Pipeline**: Download and parse scientific PDFs with structured content extraction\n",
    "- **Database Storage**: Persist paper metadata and content in PostgreSQL\n",
    "- **Error Handling**: Implement comprehensive error handling and graceful degradation\n",
    "- **Automation Ready**: Prepare components for Airflow orchestration\n",
    "\n",
    "### ðŸ”§ What We'll Test In This Notebook\n",
    "1. **arXiv API Client** - Fetch CS.AI papers with proper rate limiting\n",
    "2. **PDF Download System** - Download and cache PDFs with error handling  \n",
    "3. **Docling PDF Parser** - Extract structured content (sections, tables, figures)\n",
    "4. **Database Integration** - Store and retrieve papers from PostgreSQL\n",
    "5. **Complete Pipeline** - End-to-end processing from arXiv to database\n",
    "6. **Production Readiness** - Error handling, logging, and performance metrics\n",
    "\n",
    "\n",
    "### ðŸ“Š Success Metrics\n",
    "- arXiv API calls succeed with proper rate limiting\n",
    "- PDF download and caching works reliably  \n",
    "- Docling extracts structured content from scientific PDFs\n",
    "- Database stores complete paper metadata\n",
    "- Pipeline handles errors gracefully and continues processing\n",
    "- All components ready for Airflow automation (Week 2+)\n",
    "\n",
    "---\n",
    "\n",
    "## Week 2 Component Status\n",
    "| Component | Purpose | Status |\n",
    "|-----------|---------|--------|\n",
    "| **arXiv API Client** | Fetch CS.AI papers with rate limiting | âœ… Complete |\n",
    "| **PDF Downloader** | Download and cache PDFs locally | âœ… Complete |\n",
    "| **Docling Parser** | Extract structured content from PDFs | âœ… Complete |\n",
    "| **Metadata Fetcher** | Orchestrate complete pipeline | âœ… Complete |\n",
    "| **Database Storage** | Store papers in PostgreSQL | âš ï¸ Needs volume refresh |\n",
    "| **Airflow DAGs** | Automate daily ingestion | âš ï¸ Needs container update |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ IMPORTANT: Week 2 Database Schema Update\n",
    "\n",
    "**NEW USERS OR SCHEMA CONFLICTS**: If you're starting Week 2 fresh or experiencing database schema conflicts, use this clean start approach:\n",
    "\n",
    "### Fresh Start (Recommended for Week 2)\n",
    "```bash\n",
    "# Complete clean slate - removes all data but ensures correct schema\n",
    "docker compose down -v\n",
    "\n",
    "# Build fresh containers with latest code\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**When to use this:**\n",
    "- First time running Week 2 \n",
    "- Schema errors or column missing errors\n",
    "- Want to start with clean database\n",
    "- Previous Week 1 data not important\n",
    "\n",
    "**Note**: This destroys existing data but ensures you have the correct Week 2 schema with all new columns for PDF processing and arXiv metadata.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites Check\n",
    "\n",
    "**Before starting:**\n",
    "1. Week 1 infrastructure completed\n",
    "2. UV environment activated\n",
    "3. Docker Desktop running\n",
    "\n",
    "**Why fresh containers?** Week 2 includes new Airflow dependencies and code changes that require rebuilding images rather than using cached layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 CONTAINER & SERVICE HEALTH CHECK\n",
      "==================================================\n",
      "Project root: /Users/nishantgaurav/Project/PaperAlchemy\n",
      "\n",
      "1. Checking container status...\n",
      "âœ“ Containers are running:\n",
      "   NAME                             IMAGE                                            COMMAND                  SERVICE                 CREATED      STATUS                  PORTS\n",
      "   paperalchemy-airflow             apache/airflow:2.10.4-python3.12                 \"/usr/bin/dumb-init â€¦\"   airflow                 2 days ago   Up 2 days (healthy)     0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp\n",
      "   paperalchemy-api                 paperalchemy-api                                 \"uvicorn src.main:apâ€¦\"   api                     2 days ago   Up 2 days (healthy)     0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp\n",
      "   paperalchemy-clickhouse          clickhouse/clickhouse-server:24.8-alpine         \"/entrypoint.sh\"         clickhouse              2 days ago   Up 2 days (healthy)     8123/tcp, 9000/tcp, 9009/tcp\n",
      "   paperalchemy-dashboards          opensearchproject/opensearch-dashboards:2.19.0   \"./opensearch-dashboâ€¦\"   opensearch-dashboards   2 days ago   Up 2 days (healthy)     0.0.0.0:5602->5601/tcp, [::]:5602->5601/tcp\n",
      "   paperalchemy-langfuse            langfuse/langfuse:3                              \"dumb-init -- ./web/â€¦\"   langfuse-web            2 days ago   Up 2 days (unhealthy)   0.0.0.0:3001->3000/tcp, [::]:3001->3000/tcp\n",
      "   paperalchemy-langfuse-minio      minio/minio                                      \"sh -c 'mkdir -p /daâ€¦\"   langfuse-minio          2 days ago   Up 2 days (healthy)     0.0.0.0:9090->9000/tcp, [::]:9090->9000/tcp, 0.0.0.0:9091->9001/tcp, [::]:9091->9001/tcp\n",
      "   paperalchemy-langfuse-postgres   postgres:17                                      \"docker-entrypoint.sâ€¦\"   langfuse-postgres       2 days ago   Up 2 days (healthy)     0.0.0.0:5434->5432/tcp, [::]:5434->5432/tcp\n",
      "\n",
      "2. Checking service health...\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "\n",
      "3. Checking PostgreSQL...\n",
      "2026-01-25 19:13:41,311 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2026-01-25 19:13:41,312 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2026-01-25 19:13:41,313 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2026-01-25 19:13:41,313 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2026-01-25 19:13:41,315 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2026-01-25 19:13:41,315 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2026-01-25 19:13:41,316 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 19:13:41,318 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2026-01-25 19:13:41,318 INFO sqlalchemy.engine.Engine [generated in 0.00027s] {'table_name': 'papers', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2026-01-25 19:13:41,321 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2026-01-25 19:13:41,322 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 19:13:41,322 INFO sqlalchemy.engine.Engine SELECT 1\n",
      "2026-01-25 19:13:41,322 INFO sqlalchemy.engine.Engine [generated in 0.00030s] {}\n",
      "2026-01-25 19:13:41,323 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "âœ“ PostgreSQL: Healthy\n",
      "\n",
      "==================================================\n",
      "âœ“ ALL SERVICES HEALTHY! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Check if Fresh Containers are Built and All Services Healthy\n",
    "import subprocess\n",
    "import requests\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"WEEK 2 CONTAINER & SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week2\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    print(\"âœ— Could not find project root\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Step 1: Check if containers are built and running\n",
    "print(\"\\n1. Checking container status...\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"table\"],\n",
    "        cwd=str(project_root),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        print(\"âœ“ Containers are running:\")\n",
    "        for line in result.stdout.strip().split('\\n')[:8]:  # Show first 8 lines\n",
    "            print(f\"   {line}\")\n",
    "    else:\n",
    "        print(\"âœ— No containers running or docker compose failed\")\n",
    "        print(\"Please run the build commands from the markdown cell above\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error checking containers: {e}\")\n",
    "    print(\"Please run the build commands from the markdown cell above\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Check all service health (PaperAlchemy endpoints)\n",
    "print(\"\\n2. Checking service health...\")\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/health\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9201\",  # PaperAlchemy uses port 9201\n",
    "    \"Airflow\": \"http://localhost:8080/health\"\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âœ— {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Test PostgreSQL directly\n",
    "print(\"\\n3. Checking PostgreSQL...\")\n",
    "try:\n",
    "    from src.db.factory import make_database\n",
    "    db = make_database()\n",
    "    if db.health_check():\n",
    "        print(\"âœ“ PostgreSQL: Healthy\")\n",
    "    else:\n",
    "        print(\"âœ— PostgreSQL: Not accessible\")\n",
    "        all_healthy = False\n",
    "except Exception as e:\n",
    "    print(f\"âœ— PostgreSQL: {e}\")\n",
    "    all_healthy = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if all_healthy:\n",
    "    print(\"âœ“ ALL SERVICES HEALTHY! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"âœ— Some services need attention.\")\n",
    "    print(\"If you just rebuilt containers, wait 1-2 minutes and run this cell again.\")\n",
    "    print(\"Airflow and OpenSearch take longest to start up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.7\n",
      "Environment: /Users/nishantgaurav/Project/PaperAlchemy/.venv/bin/python\n",
      "âœ“ Project root: /Users/nishantgaurav/Project/PaperAlchemy\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "# Find project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week2\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"âœ“ Project root: {project_root}\")\n",
    "    # Add project to Python path\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"âœ— Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Health Verification\n",
    "\n",
    "Ensure all services from Week 1 are still running correctly:\n",
    "\n",
    "### ðŸ”— Service Access Points\n",
    "- **FastAPI**: http://localhost:8000/docs (API documentation)\n",
    "- **PostgreSQL**: via API or `docker exec -it rag-postgres psql -U rag_user -d rag_db`\n",
    "- **OpenSearch**: http://localhost:9200/_cluster/health\n",
    "- **Ollama**: http://localhost:11434 (LLM service)\n",
    "- **Airflow**: http://localhost:8080 (Username: `admin`, Password: `admin`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 2 PREREQUISITE CHECK\n",
      "==================================================\n",
      "âœ“ FastAPI: Healthy\n",
      "âœ“ Ollama: Healthy\n",
      "âœ“ OpenSearch: Healthy\n",
      "âœ“ Airflow: Healthy\n",
      "\n",
      "Checking PostgreSQL...\n",
      "2026-01-25 19:13:54,036 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 19:13:54,037 INFO sqlalchemy.engine.Engine SELECT 1\n",
      "2026-01-25 19:13:54,037 INFO sqlalchemy.engine.Engine [cached since 12.72s ago] {}\n",
      "2026-01-25 19:13:54,043 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "âœ“ PostgreSQL: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 2 development.\n"
     ]
    }
   ],
   "source": [
    "# Test Service Connectivity\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/health\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"OpenSearch\": \"http://localhost:9201\",  # PaperAlchemy uses port 9201\n",
    "    \"Airflow\": \"http://localhost:8080/health\"  \n",
    "}\n",
    "\n",
    "print(\"WEEK 2 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ“ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"âœ— {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"âœ— {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Test PostgreSQL directly\n",
    "print(\"\\nChecking PostgreSQL...\")\n",
    "try:\n",
    "    from src.db.factory import make_database\n",
    "    db = make_database()\n",
    "    if db.health_check():\n",
    "        print(\"âœ“ PostgreSQL: Healthy\")\n",
    "    else:\n",
    "        print(\"âœ— PostgreSQL: Not accessible\")\n",
    "        all_healthy = False\n",
    "except Exception as e:\n",
    "    print(f\"âœ— PostgreSQL: {e}\")\n",
    "    all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 2 development.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Check Week 1 notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. arXiv API Client Testing\n",
    "\n",
    "Test the arXiv API client with rate limiting and retry logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING ARXIV API CLIENT\n",
      "========================================\n",
      "âœ“ Client created: http://export.arxiv.org/api/query\n",
      "   Rate limit: 3.0s\n",
      "   Max results: 100\n",
      "   Category: cs.AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test arXiv API Client\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import our arXiv client\n",
    "from src.services.arxiv.factory import make_arxiv_client\n",
    "\n",
    "print(\"TESTING ARXIV API CLIENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create client\n",
    "arxiv_client = make_arxiv_client()\n",
    "print(f\"âœ“ Client created: {arxiv_client.base_url}\")\n",
    "print(f\"   Rate limit: {arxiv_client.rate_limit_delay}s\")\n",
    "print(f\"   Max results: {arxiv_client.max_results}\")\n",
    "print(f\"   Category: {arxiv_client.search_category}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Fetch Recent CS.AI Papers\n",
      "âœ“ Fetched 2 papers\n",
      "   1. [2601.16211] Why Can't I Open My Drawer? Mitigating Object-Driven Shortcu...\n",
      "      Authors: Geo Ahn, Inwoong Lee...\n",
      "      Categories: cs.CV, cs.AI\n",
      "      Published: 2026-01-22T18:59:13Z\n",
      "\n",
      "   2. [2601.16210] PyraTok: Language-Aligned Pyramidal Tokenizer for Video Unde...\n",
      "      Authors: Onkar Susladkar, Tushar Prakash...\n",
      "      Categories: cs.CV, cs.AI\n",
      "      Published: 2026-01-22T18:58:55Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Paper Fetching\n",
    "async def test_paper_fetching():\n",
    "    \"\"\"Test fetching papers from arXiv with rate limiting.\"\"\"\n",
    "    \n",
    "    print(\"Test 1: Fetch Recent CS.AI Papers\")\n",
    "    try:\n",
    "        papers = await arxiv_client.fetch_papers(\n",
    "            max_results=2, \n",
    "            sort_by=\"submittedDate\",\n",
    "            sort_order=\"descending\"\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Fetched {len(papers)} papers\")\n",
    "        \n",
    "        if papers:\n",
    "            for i, paper in enumerate(papers[:2], 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "        \n",
    "        return papers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error fetching papers: {e}\")\n",
    "        if \"503\" in str(e):\n",
    "            print(\"   arXiv API temporarily unavailable (normal)\")\n",
    "            print(\"   Rate limiting and error handling working correctly\")\n",
    "        return []\n",
    "\n",
    "# Run the test\n",
    "papers = await test_paper_fetching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Date Range Filtering\n",
      "âœ“ Date filtering test: 5 papers from 20250808-20250809\n",
      "   1. [2508.07111] Investigating Intersectional Bias in Large Language Models u...\n",
      "      Authors: Falaah Arif Khan, Nivedha Sivakumar...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T22:24:40Z\n",
      "\n",
      "   2. [2508.07107] Designing a Feedback-Driven Decision Support System for Dyna...\n",
      "      Authors: Timothy Oluwapelumi Adeyemi, Nadiah Fahad AlOtaibi\n",
      "      Categories: cs.AI, cs.CY\n",
      "      Published: 2025-08-09T21:24:54Z\n",
      "\n",
      "   3. [2508.07102] Towards High-Order Mean Flow Generative Models: Feasibility,...\n",
      "      Authors: Yang Cao, Yubin Chen...\n",
      "      Categories: cs.LG, cs.AI, cs.CV\n",
      "      Published: 2025-08-09T21:10:58Z\n",
      "\n",
      "   4. [2508.07101] Less Is More: Training-Free Sparse Attention with Global Loc...\n",
      "      Authors: Lijie Yang, Zhihao Zhang...\n",
      "      Categories: cs.CL, cs.AI\n",
      "      Published: 2025-08-09T21:10:33Z\n",
      "\n",
      "   5. [2508.07095] Hide or Highlight: Understanding the Impact of Factuality Ex...\n",
      "      Authors: Hyo Jin Do, Werner Geyer\n",
      "      Categories: cs.HC, cs.AI\n",
      "      Published: 2025-08-09T20:45:21Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Date Filtering\n",
    "async def test_date_filtering():\n",
    "    \"\"\"Test date range filtering functionality.\"\"\"\n",
    "    \n",
    "    print(\"Test 2: Date Range Filtering\")\n",
    "    \n",
    "    # Use specific dates: \n",
    "    from_date = \"20250808\"  \n",
    "    to_date = \"20250809\"    \n",
    "    try:\n",
    "        date_papers = await arxiv_client.fetch_papers(\n",
    "            max_results=5,\n",
    "            from_date=from_date,\n",
    "            to_date=to_date\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Date filtering test: {len(date_papers)} papers from {from_date}-{to_date}\")\n",
    "        \n",
    "        if date_papers:\n",
    "            for i, paper in enumerate(date_papers, 1):\n",
    "                print(f\"   {i}. [{paper.arxiv_id}] {paper.title[:60]}...\")\n",
    "                print(f\"      Authors: {', '.join(paper.authors[:2])}{'...' if len(paper.authors) > 2 else ''}\")\n",
    "                print(f\"      Categories: {', '.join(paper.categories)}\")\n",
    "                print(f\"      Published: {paper.published_date}\")\n",
    "                print()\n",
    "        \n",
    "        return date_papers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Date filtering error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run date filtering test\n",
    "date_papers = await test_date_filtering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Download and Caching\n",
    "\n",
    "Test PDF download functionality with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: PDF Download & Caching\n",
      "Testing PDF download for: 2508.07111\n",
      "Title: Investigating Intersectional Bias in Large Language Models u...\n",
      "âœ“ PDF downloaded: 2508.07111.pdf (6.81 MB)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Download\n",
    "async def test_pdf_download(test_papers):\n",
    "    \"\"\"Test PDF downloading with caching.\"\"\"\n",
    "\n",
    "    print(\"Test 3: PDF Download & Caching\")\n",
    "    \n",
    "    if not test_papers:\n",
    "        print(\"No papers available for PDF download test\")\n",
    "        return None\n",
    "    \n",
    "    # Test with first paper\n",
    "    test_paper = test_papers[0]\n",
    "    print(f\"Testing PDF download for: {test_paper.arxiv_id}\")\n",
    "    print(f\"Title: {test_paper.title[:60]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Download PDF \n",
    "        pdf_path = await arxiv_client.download_pdf(test_paper)\n",
    "        \n",
    "        if pdf_path and pdf_path.exists():\n",
    "            size_mb = pdf_path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"âœ“ PDF downloaded: {pdf_path.name} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            return pdf_path\n",
    "        else:\n",
    "            print(\"âœ— PDF download failed\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— PDF download error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run PDF download test \n",
    "pdf_path = await test_pdf_download(date_papers[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docling PDF Processing\n",
    "\n",
    "Test PDF parsing with Docling for structured content extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4: PDF Parsing with Docling\n",
      "========================================\n",
      "PDF parser service created\n",
      "Config: 50 pages, 50MB\n",
      "\n",
      "Found 4 PDF files to test parsing\n",
      "Testing PDF parsing with: 2508.11110.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PDF parsing successful!\n",
      "  Sections: 1\n",
      "  Raw text length: 40584 characters\n",
      "  Parser used: docling\n",
      "  First section: 'Introduction' (37800 chars)\n"
     ]
    }
   ],
   "source": [
    "# Test PDF Parsing with Docling\n",
    "from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "from src.config import get_settings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Test 4: PDF Parsing with Docling\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create PDF parser\n",
    "pdf_parser = make_pdf_parser_service()\n",
    "settings = get_settings()\n",
    "print(\"PDF parser service created\")\n",
    "print(f\"Config: {settings.pdf_parser.max_pages} pages, {settings.pdf_parser.max_file_size_mb}MB\")\n",
    "\n",
    "# Test parsing with actual PDF files\n",
    "cache_dir = Path(\"data/arxiv_pdfs\")\n",
    "if cache_dir.exists():\n",
    "    pdf_files = list(cache_dir.glob(\"*.pdf\"))\n",
    "    print(f\"\\nFound {len(pdf_files)} PDF files to test parsing\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        # Test parsing the first PDF\n",
    "        test_pdf = pdf_files[0]\n",
    "        print(f\"Testing PDF parsing with: {test_pdf.name}\")\n",
    "        \n",
    "        try:\n",
    "            pdf_content = await pdf_parser.parse_pdf(test_pdf)\n",
    "            \n",
    "            if pdf_content:\n",
    "                print(f\"âœ“ PDF parsing successful!\")\n",
    "                print(f\"  Sections: {len(pdf_content.sections)}\")\n",
    "                print(f\"  Raw text length: {len(pdf_content.raw_text)} characters\")\n",
    "                print(f\"  Parser used: {pdf_content.parser_used}\")\n",
    "                \n",
    "                # Show first section as example\n",
    "                if pdf_content.sections:\n",
    "                    first_section = pdf_content.sections[0]\n",
    "                    print(f\"  First section: '{first_section.title}' ({len(first_section.content)} chars)\")\n",
    "            else:\n",
    "                print(\"âœ— PDF parsing failed (Docling compatibility issue)\")\n",
    "                print(\"This is expected - not all PDFs work with Docling\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— PDF parsing error: {e}\")\n",
    "            print(\"This demonstrates the error handling in action\")\n",
    "    else:\n",
    "        print(\"No PDF files available for parsing test\")\n",
    "else:\n",
    "    print(\"No PDF cache directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Storage Testing\n",
    "\n",
    "Test storing papers in PostgreSQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5: Database Storage\n",
      "========================================\n",
      "âœ“ Database connection created\n",
      "Storing paper: 2601.16211\n",
      "2026-01-25 19:15:04,287 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 19:15:04,294 INFO sqlalchemy.engine.Engine INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at\n",
      "2026-01-25 19:15:04,295 INFO sqlalchemy.engine.Engine [no key 0.00156s] {'arxiv_id': '2601.16211', 'title': \"Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition\", 'authors': '[\"Geo Ahn\", \"Inwoong Lee\", \"Taeoh Kim\", \"Minho Shim\", \"Dongyoon Wee\", \"Jinwoo Choi\"]', 'abstract': 'We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations ... (1253 characters truncated) ... ven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.', 'categories': '[\"cs.CV\", \"cs.AI\"]', 'published_date': datetime.datetime(2026, 1, 22, 18, 59, 13, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2601.16211.pdf', 'pdf_content': None, 'sections': 'null', 'parsing_status': 'pending', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 4, 238652)}\n",
      "âœ“ Paper stored with ID: 1\n",
      "   Database ID: 1\n",
      "   arXiv ID: 2601.16211\n",
      "   Title: Why Can't I Open My Drawer? Mitigating Object-Driv...\n",
      "   Authors: 6 authors\n",
      "   Categories: cs.CV, cs.AI\n",
      "2026-01-25 19:15:04,308 INFO sqlalchemy.engine.Engine SELECT papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at \n",
      "FROM papers \n",
      "WHERE papers.arxiv_id = %(arxiv_id_1)s\n",
      "2026-01-25 19:15:04,308 INFO sqlalchemy.engine.Engine [generated in 0.00043s] {'arxiv_id_1': '2601.16211'}\n",
      "âœ“ Paper retrieval test passed\n",
      "2026-01-25 19:15:04,313 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# Test Database Storage\n",
    "from src.db.factory import make_database\n",
    "from src.repositories.paper import PaperRepository\n",
    "from src.schemas.arxiv.paper import PaperCreate\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "print(\"Test 5: Database Storage\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create database connection\n",
    "database = make_database()\n",
    "print(\"âœ“ Database connection created\")\n",
    "\n",
    "if papers:\n",
    "    test_paper = papers[0]\n",
    "    print(f\"Storing paper: {test_paper.arxiv_id}\")\n",
    "    \n",
    "    try:\n",
    "        with database.get_session() as session:\n",
    "            paper_repo = PaperRepository(session)\n",
    "            \n",
    "            # Convert to database format\n",
    "            published_date = date_parser.parse(test_paper.published_date) if isinstance(test_paper.published_date, str) else test_paper.published_date\n",
    "            \n",
    "            paper_create = PaperCreate(\n",
    "                arxiv_id=test_paper.arxiv_id,\n",
    "                title=test_paper.title,\n",
    "                authors=test_paper.authors,\n",
    "                abstract=test_paper.abstract,\n",
    "                categories=test_paper.categories,\n",
    "                published_date=published_date,\n",
    "                pdf_url=test_paper.pdf_url\n",
    "            )\n",
    "            \n",
    "            # Store paper (upsert to avoid duplicates)\n",
    "            stored_paper = paper_repo.upsert(paper_create)\n",
    "            \n",
    "            if stored_paper:\n",
    "                print(f\"âœ“ Paper stored with ID: {stored_paper.id}\")\n",
    "                print(f\"   Database ID: {stored_paper.id}\")\n",
    "                print(f\"   arXiv ID: {stored_paper.arxiv_id}\")\n",
    "                print(f\"   Title: {stored_paper.title[:50]}...\")\n",
    "                print(f\"   Authors: {len(stored_paper.authors)} authors\")\n",
    "                print(f\"   Categories: {', '.join(stored_paper.categories)}\")\n",
    "                \n",
    "                # Test retrieval\n",
    "                retrieved_paper = paper_repo.get_by_arxiv_id(test_paper.arxiv_id)\n",
    "                if retrieved_paper:\n",
    "                    print(f\"âœ“ Paper retrieval test passed\")\n",
    "                else:\n",
    "                    print(f\"âœ— Paper retrieval failed\")\n",
    "            else:\n",
    "                print(\"âœ— Paper storage failed\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Database error: {e}\")\n",
    "else:\n",
    "    print(\"No papers available for database storage test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6: Complete Metadata Fetcher Pipeline\n",
      "==================================================\n",
      "âœ“ Metadata fetcher service created\n",
      "Running small batch test (2 papers, no PDF processing for speed)...\n",
      "2026-01-25 19:15:07,134 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 19:15:07,137 INFO sqlalchemy.engine.Engine INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at\n",
      "2026-01-25 19:15:07,137 INFO sqlalchemy.engine.Engine [no key 0.00055s] {'arxiv_id': '2601.16211', 'title': \"Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition\", 'authors': '[\"Geo Ahn\", \"Inwoong Lee\", \"Taeoh Kim\", \"Minho Shim\", \"Dongyoon Wee\", \"Jinwoo Choi\"]', 'abstract': 'We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations ... (1253 characters truncated) ... ven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.', 'categories': '[\"cs.CV\", \"cs.AI\"]', 'published_date': datetime.datetime(2026, 1, 22, 18, 59, 13, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2601.16211.pdf', 'pdf_content': None, 'sections': 'null', 'parsing_status': 'pending', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 7, 133397)}\n",
      "2026-01-25 19:15:07,142 INFO sqlalchemy.engine.Engine INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at\n",
      "2026-01-25 19:15:07,143 INFO sqlalchemy.engine.Engine [no key 0.00084s] {'arxiv_id': '2601.16210', 'title': 'PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation', 'authors': '[\"Onkar Susladkar\", \"Tushar Prakash\", \"Adheesh Juvekar\", \"Kiet A. Nguyen\", \"Dong-Hwan Jang\", \"Inderjit S Dhillon\", \"Ismini Lourentzou\"]', 'abstract': 'Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebook ... (870 characters truncated) ... SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.', 'categories': '[\"cs.CV\", \"cs.AI\"]', 'published_date': datetime.datetime(2026, 1, 22, 18, 58, 55, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2601.16210.pdf', 'pdf_content': None, 'sections': 'null', 'parsing_status': 'pending', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 7, 141749)}\n",
      "2026-01-25 19:15:07,149 INFO sqlalchemy.engine.Engine COMMIT\n",
      "\n",
      "PIPELINE RESULTS:\n",
      "   Papers fetched: 2\n",
      "   PDFs downloaded: 0\n",
      "   PDFs parsed: 0\n",
      "   Papers stored: 2\n",
      "   Processing time: 0.3s\n",
      "   Errors: 0\n",
      "\n",
      "âœ“ Pipeline test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test Complete Pipeline\n",
    "from src.services.metadata_fetcher import make_metadata_fetcher\n",
    "\n",
    "print(\"Test 6: Complete Metadata Fetcher Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create metadata fetcher\n",
    "metadata_fetcher = make_metadata_fetcher(arxiv_client, pdf_parser)\n",
    "print(\"âœ“ Metadata fetcher service created\")\n",
    "\n",
    "# Test with small batch\n",
    "print(\"Running small batch test (2 papers, no PDF processing for speed)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=2,  \n",
    "            process_pdfs=False,  \n",
    "            store_to_db=True,\n",
    "            db_session=session\n",
    "        )\n",
    "    \n",
    "    print(\"\\nPIPELINE RESULTS:\")\n",
    "    print(f\"   Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"   PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"   PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"   Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"   Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"   Errors: {len(results.get('errors', []))}\")\n",
    "    \n",
    "    if results.get('errors'):\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in results.get('errors', [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        print(\"\\nâœ“ Pipeline test successful!\")\n",
    "    else:\n",
    "        print(\"\\nNo papers fetched - may be arXiv API unavailability\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7: Airflow DAG Status\n",
      "========================================\n",
      "  Airflow UI Access:\n",
      "   URL: http://localhost:8080\n",
      "   Username: admin\n",
      "   Password: admin\n",
      "\n",
      "Available DAGs:\n",
      "   No arXiv or hello DAGs found yet\n",
      "   DAGs will be added in later steps\n",
      "\n",
      "âœ“ No DAG import errors found\n",
      "\n",
      "  To view DAGs graphically:\n",
      "   1. Open http://localhost:8080 in your browser\n",
      "   2. Login with admin/admin\n",
      "   3. Click on 'arxiv_paper_ingestion' DAG to see the workflow\n"
     ]
    }
   ],
   "source": [
    "# Test Airflow DAGs\n",
    "print(\"Test 7: Airflow DAG Status\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"  Airflow UI Access:\")\n",
    "print(\"   URL: http://localhost:8080\")\n",
    "print(\"   Username: admin\")\n",
    "print(\"   Password: admin\")\n",
    "print()\n",
    "\n",
    "# Check DAG status using docker exec (PaperAlchemy container name)\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"exec\", \"paperalchemy-airflow\", \"airflow\", \"dags\", \"list\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.strip().split('\\n')\n",
    "        dag_lines = [line for line in lines if 'arxiv' in line.lower() or 'hello' in line.lower()]\n",
    "        \n",
    "        print(\"Available DAGs:\")\n",
    "        for line in dag_lines:\n",
    "            if '|' in line:\n",
    "                parts = [part.strip() for part in line.split('|')]\n",
    "                if len(parts) >= 3:\n",
    "                    dag_id = parts[0]\n",
    "                    is_paused = parts[2]\n",
    "                    status = \"Active\" if is_paused == \"False\" else \"Paused\"\n",
    "                    print(f\"   - {dag_id}: {status}\")\n",
    "        \n",
    "        if not dag_lines:\n",
    "            print(\"   No arXiv or hello DAGs found yet\")\n",
    "            print(\"   DAGs will be added in later steps\")\n",
    "        \n",
    "        # Check for import errors\n",
    "        error_result = subprocess.run(\n",
    "            [\"docker\", \"exec\", \"paperalchemy-airflow\", \"airflow\", \"dags\", \"list-import-errors\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if \"docling\" in error_result.stderr:\n",
    "            print(\"\\nKnown Issue: Docling not installed in Airflow container\")\n",
    "            print(\"   - This is expected for Week 2\")\n",
    "            print(\"   - DAG structure is complete, runtime needs container fix\")\n",
    "            print(\"   - Solution: Add docling to Airflow container startup\")\n",
    "        elif error_result.returncode == 0:\n",
    "            print(\"\\nâœ“ No DAG import errors found\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âœ— Could not list DAGs: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Airflow test error: {e}\")\n",
    "\n",
    "print(\"\\n  To view DAGs graphically:\")\n",
    "print(\"   1. Open http://localhost:8080 in your browser\")\n",
    "print(\"   2. Login with admin/admin\")\n",
    "print(\"   3. Click on 'arxiv_paper_ingestion' DAG to see the workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 8: Complete Pipeline with PDF Processing\n",
      "==================================================\n",
      "âœ“ Using metadata fetcher service from previous test\n",
      "Running enhanced test (3 papers with PDF processing)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:15:33,341 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 19:15:33,342 INFO sqlalchemy.engine.Engine INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at\n",
      "2026-01-25 19:15:33,343 INFO sqlalchemy.engine.Engine [no key 0.00070s] {'arxiv_id': '2508.11121', 'title': 'Tabularis Formatus: Predictive Formatting for Tables', 'authors': '[\"Mukul Singh\", \"Jos\\\\u00e9 Cambronero\", \"Sumit Gulwani\", \"Vu Le\", \"Gust Verbruggen\"]', 'abstract': 'Spreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional formatting (CF) r ... (1192 characters truncated) ... d complete formatting suggestions than current systems and outperforms these by 15.6\\\\%--26.5\\\\% on matching user added ground truth rules in tables.', 'categories': '[\"cs.DB\", \"cs.AI\", \"cs.SE\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 54, 40, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11121.pdf', 'pdf_content': '## Abstract\\n\\nSpreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional fo ... (84816 characters truncated) ... ng Zhang. 2024. NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries. arXiv:2402.14853 [cs.CL] https: //arxiv.org/abs/2402.14853', 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11121v1  [cs.DB]  14 Aug 2025\\\\nAbstract\\\\nSpreadsheet manipulation software are widely used for da ... (80593 characters truncated) ... 2Formula: Generating Spreadsheet Formulas from Natural Language Queries. arXiv:2402.14853 [cs.CL] https: //arxiv.org/abs/2402.14853\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 33, 337213)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing 2508.11121: (psycopg2.errors.UntranslatableCharacter) unsupported Unicode escape sequence\n",
      "LINE 1280: ...14853 [cs.CL] https: //arxiv.org/abs/2402.14853', '[{\"title\"...\n",
      "                                                                ^\n",
      "DETAIL:  \\u0000 cannot be converted to text.\n",
      "CONTEXT:  JSON data, line 1: ...\\udc58 ). Tafo outperforms both baselines\\n\\u0000...\n",
      "\n",
      "[SQL: INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at]\n",
      "[parameters: {'arxiv_id': '2508.11121', 'title': 'Tabularis Formatus: Predictive Formatting for Tables', 'authors': '[\"Mukul Singh\", \"Jos\\\\u00e9 Cambronero\", \"Sumit Gulwani\", \"Vu Le\", \"Gust Verbruggen\"]', 'abstract': 'Spreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional formatting (CF) r ... (1192 characters truncated) ... d complete formatting suggestions than current systems and outperforms these by 15.6\\\\%--26.5\\\\% on matching user added ground truth rules in tables.', 'categories': '[\"cs.DB\", \"cs.AI\", \"cs.SE\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 54, 40, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11121.pdf', 'pdf_content': '## Abstract\\n\\nSpreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional fo ... (84816 characters truncated) ... ng Zhang. 2024. NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries. arXiv:2402.14853 [cs.CL] https: //arxiv.org/abs/2402.14853', 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11121v1  [cs.DB]  14 Aug 2025\\\\nAbstract\\\\nSpreadsheet manipulation software are widely used for da ... (80593 characters truncated) ... 2Formula: Generating Spreadsheet Formulas from Natural Language Queries. arXiv:2402.14853 [cs.CL] https: //arxiv.org/abs/2402.14853\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 33, 337213)}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/9h9h)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:15:41,774 INFO sqlalchemy.engine.Engine INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at\n",
      "2026-01-25 19:15:41,775 INFO sqlalchemy.engine.Engine [no key 0.00088s] {'arxiv_id': '2508.11112', 'title': 'Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees', 'authors': '[\"Jianhao Ma\", \"Lin Xiao\"]', 'abstract': 'Optimization problems over discrete or quantized variables are very challenging in general due to the combinatorial nature of their search space. Pie ... (916 characters truncated) ... ations of $\\\\ell_1$-, squared $\\\\ell_2$-, and nonconvex regularizations using PAR and obtain similar statistical guarantees with quantized solutions.', 'categories': '[\"cs.LG\", \"cs.AI\", \"math.OC\", \"stat.ML\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 35, 21, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11112.pdf', 'pdf_content': \"## Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees\\n\\nJianhao Ma University of Pennsylvania jianhaom@wh ... (68913 characters truncated) ... , if x âˆˆ [ q k + q k +1 2 , q k +1 ] for some k , its proximal mapping can be derived by\\n\\n<!-- formula-not-decoded -->\\n\\nThis completes the proof.\", 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11112v1  [cs.LG]  14 Aug 2025\\\\nQuantization through Piecewise-Affine Regularization: Optimization  ... (76970 characters truncated) ... y, if x \\\\u2208 [ q k + q k +1 2 , q k +1 ] for some k , its proximal mapping can be derived by\\\\n\\\\nThis completes the proof.\\\\n30\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 41, 772849)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing 2508.11112: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "[SQL: INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at]\n",
      "[parameters: {'arxiv_id': '2508.11112', 'title': 'Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees', 'authors': '[\"Jianhao Ma\", \"Lin Xiao\"]', 'abstract': 'Optimization problems over discrete or quantized variables are very challenging in general due to the combinatorial nature of their search space. Pie ... (916 characters truncated) ... ations of $\\\\ell_1$-, squared $\\\\ell_2$-, and nonconvex regularizations using PAR and obtain similar statistical guarantees with quantized solutions.', 'categories': '[\"cs.LG\", \"cs.AI\", \"math.OC\", \"stat.ML\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 35, 21, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11112.pdf', 'pdf_content': \"## Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees\\n\\nJianhao Ma University of Pennsylvania jianhaom@wh ... (68913 characters truncated) ... , if x âˆˆ [ q k + q k +1 2 , q k +1 ] for some k , its proximal mapping can be derived by\\n\\n<!-- formula-not-decoded -->\\n\\nThis completes the proof.\", 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11112v1  [cs.LG]  14 Aug 2025\\\\nQuantization through Piecewise-Affine Regularization: Optimization  ... (76970 characters truncated) ... y, if x \\\\u2208 [ q k + q k +1 2 , q k +1 ] for some k , its proximal mapping can be derived by\\\\n\\\\nThis completes the proof.\\\\n30\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 41, 772849)}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:15:49,265 INFO sqlalchemy.engine.Engine INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at\n",
      "2026-01-25 19:15:49,265 INFO sqlalchemy.engine.Engine [no key 0.00052s] {'arxiv_id': '2508.11110', 'title': 'Diffusion is a code repair operator and generator', 'authors': '[\"Mukul Singh\", \"Gust Verbruggen\", \"Vu Le\", \"Sumit Gulwani\"]', 'abstract': 'Code diffusion models generate code by iteratively removing noise from the latent representation of a code snippet. During later steps of the diffusi ... (770 characters truncated) ... om the diffusion process. We perform experiments on 3 domains (Python, Excel and PowerShell) to evaluate applications, as well as analyze properties.', 'categories': '[\"cs.SE\", \"cs.AI\", \"cs.CL\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 27, 9, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11110.pdf', 'pdf_content': '## Diffusion is a code repair operator and generator\\n\\nMukul Singh Microsoft Redmond, WA\\n\\nsinghmukul@microsoft.com\\n\\nVu Le Microsoft Redmond, WA  ... (40622 characters truncated) ...  with natural language. 17(3):497-510, November 2023b. ISSN 2150-8097. doi: 10.14778/ 3632093.3632111. URL https://doi.org/10.14778/3632093.3632111 .', 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11110v1  [cs.SE]  14 Aug 2025\\\\nDiffusion is a code repair operator and generator\\\\nMukul Singh Mic ... (43187 characters truncated) ...  17(3):497-510, November 2023b. ISSN 2150-8097. doi: 10.14778/ 3632093.3632111. URL https://doi.org/10.14778/3632093.3632111 .\\\\n12\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 49, 263848)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing 2508.11110: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "[SQL: INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at]\n",
      "[parameters: {'arxiv_id': '2508.11110', 'title': 'Diffusion is a code repair operator and generator', 'authors': '[\"Mukul Singh\", \"Gust Verbruggen\", \"Vu Le\", \"Sumit Gulwani\"]', 'abstract': 'Code diffusion models generate code by iteratively removing noise from the latent representation of a code snippet. During later steps of the diffusi ... (770 characters truncated) ... om the diffusion process. We perform experiments on 3 domains (Python, Excel and PowerShell) to evaluate applications, as well as analyze properties.', 'categories': '[\"cs.SE\", \"cs.AI\", \"cs.CL\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 27, 9, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11110.pdf', 'pdf_content': '## Diffusion is a code repair operator and generator\\n\\nMukul Singh Microsoft Redmond, WA\\n\\nsinghmukul@microsoft.com\\n\\nVu Le Microsoft Redmond, WA  ... (40622 characters truncated) ...  with natural language. 17(3):497-510, November 2023b. ISSN 2150-8097. doi: 10.14778/ 3632093.3632111. URL https://doi.org/10.14778/3632093.3632111 .', 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11110v1  [cs.SE]  14 Aug 2025\\\\nDiffusion is a code repair operator and generator\\\\nMukul Singh Mic ... (43187 characters truncated) ...  17(3):497-510, November 2023b. ISSN 2150-8097. doi: 10.14778/ 3632093.3632111. URL https://doi.org/10.14778/3632093.3632111 .\\\\n12\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 49, 263848)}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 19:15:49,277 INFO sqlalchemy.engine.Engine COMMIT\n",
      "\n",
      "ENHANCED PIPELINE RESULTS:\n",
      "   Papers fetched: 3\n",
      "   PDFs downloaded: 3\n",
      "   PDFs parsed: 3\n",
      "   Papers stored: 0\n",
      "   Processing time: 30.9s\n",
      "   Errors: 3\n",
      "   Download success rate: 100.0%\n",
      "   Parse success rate: 100.0%\n",
      "\n",
      "Errors encountered (showing graceful error handling):\n",
      "   - Error processing 2508.11121: (psycopg2.errors.UntranslatableCharacter) unsupported Unicode escape sequence\n",
      "LINE 1280: ...14853 [cs.CL] https: //arxiv.org/abs/2402.14853', '[{\"title\"...\n",
      "                                                                ^\n",
      "DETAIL:  \\u0000 cannot be converted to text.\n",
      "CONTEXT:  JSON data, line 1: ...\\udc58 ). Tafo outperforms both baselines\\n\\u0000...\n",
      "\n",
      "[SQL: INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at]\n",
      "[parameters: {'arxiv_id': '2508.11121', 'title': 'Tabularis Formatus: Predictive Formatting for Tables', 'authors': '[\"Mukul Singh\", \"Jos\\\\u00e9 Cambronero\", \"Sumit Gulwani\", \"Vu Le\", \"Gust Verbruggen\"]', 'abstract': 'Spreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional formatting (CF) r ... (1192 characters truncated) ... d complete formatting suggestions than current systems and outperforms these by 15.6\\\\%--26.5\\\\% on matching user added ground truth rules in tables.', 'categories': '[\"cs.DB\", \"cs.AI\", \"cs.SE\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 54, 40, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11121.pdf', 'pdf_content': '## Abstract\\n\\nSpreadsheet manipulation software are widely used for data management and analysis of tabular data, yet the creation of conditional fo ... (84816 characters truncated) ... ng Zhang. 2024. NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries. arXiv:2402.14853 [cs.CL] https: //arxiv.org/abs/2402.14853', 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11121v1  [cs.DB]  14 Aug 2025\\\\nAbstract\\\\nSpreadsheet manipulation software are widely used for da ... (80593 characters truncated) ... 2Formula: Generating Spreadsheet Formulas from Natural Language Queries. arXiv:2402.14853 [cs.CL] https: //arxiv.org/abs/2402.14853\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 33, 337213)}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/9h9h)\n",
      "   - Error processing 2508.11112: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "[SQL: INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at]\n",
      "[parameters: {'arxiv_id': '2508.11112', 'title': 'Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees', 'authors': '[\"Jianhao Ma\", \"Lin Xiao\"]', 'abstract': 'Optimization problems over discrete or quantized variables are very challenging in general due to the combinatorial nature of their search space. Pie ... (916 characters truncated) ... ations of $\\\\ell_1$-, squared $\\\\ell_2$-, and nonconvex regularizations using PAR and obtain similar statistical guarantees with quantized solutions.', 'categories': '[\"cs.LG\", \"cs.AI\", \"math.OC\", \"stat.ML\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 35, 21, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11112.pdf', 'pdf_content': \"## Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees\\n\\nJianhao Ma University of Pennsylvania jianhaom@wh ... (68913 characters truncated) ... , if x âˆˆ [ q k + q k +1 2 , q k +1 ] for some k , its proximal mapping can be derived by\\n\\n<!-- formula-not-decoded -->\\n\\nThis completes the proof.\", 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11112v1  [cs.LG]  14 Aug 2025\\\\nQuantization through Piecewise-Affine Regularization: Optimization  ... (76970 characters truncated) ... y, if x \\\\u2208 [ q k + q k +1 2 , q k +1 ] for some k , its proximal mapping can be derived by\\\\n\\\\nThis completes the proof.\\\\n30\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 41, 772849)}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "   - Error processing 2508.11110: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "[SQL: INSERT INTO papers (arxiv_id, title, authors, abstract, categories, published_date, updated_date, pdf_url, pdf_content, sections, parsing_status, parsing_error) VALUES (%(arxiv_id)s, %(title)s, %(authors)s::JSONB, %(abstract)s, %(categories)s::JSONB, %(published_date)s, %(updated_date)s, %(pdf_url)s, %(pdf_content)s, %(sections)s::JSONB, %(parsing_status)s, %(parsing_error)s) ON CONFLICT (arxiv_id) DO UPDATE SET title = excluded.title, authors = excluded.authors, abstract = excluded.abstract, categories = excluded.categories, updated_date = excluded.updated_date, pdf_content = excluded.pdf_content, sections = excluded.sections, parsing_status = excluded.parsing_status, parsing_error = excluded.parsing_error, updated_at = %(param_1)s RETURNING papers.id, papers.arxiv_id, papers.title, papers.authors, papers.abstract, papers.categories, papers.published_date, papers.updated_date, papers.pdf_url, papers.pdf_content, papers.sections, papers.parsing_status, papers.parsing_error, papers.created_at, papers.updated_at]\n",
      "[parameters: {'arxiv_id': '2508.11110', 'title': 'Diffusion is a code repair operator and generator', 'authors': '[\"Mukul Singh\", \"Gust Verbruggen\", \"Vu Le\", \"Sumit Gulwani\"]', 'abstract': 'Code diffusion models generate code by iteratively removing noise from the latent representation of a code snippet. During later steps of the diffusi ... (770 characters truncated) ... om the diffusion process. We perform experiments on 3 domains (Python, Excel and PowerShell) to evaluate applications, as well as analyze properties.', 'categories': '[\"cs.SE\", \"cs.AI\", \"cs.CL\"]', 'published_date': datetime.datetime(2025, 8, 14, 23, 27, 9, tzinfo=tzutc()), 'updated_date': None, 'pdf_url': 'https://arxiv.org/pdf/2508.11110.pdf', 'pdf_content': '## Diffusion is a code repair operator and generator\\n\\nMukul Singh Microsoft Redmond, WA\\n\\nsinghmukul@microsoft.com\\n\\nVu Le Microsoft Redmond, WA  ... (40622 characters truncated) ...  with natural language. 17(3):497-510, November 2023b. ISSN 2150-8097. doi: 10.14778/ 3632093.3632111. URL https://doi.org/10.14778/3632093.3632111 .', 'sections': '[{\"title\": \"Introduction\", \"content\": \"arXiv:2508.11110v1  [cs.SE]  14 Aug 2025\\\\nDiffusion is a code repair operator and generator\\\\nMukul Singh Mic ... (43187 characters truncated) ...  17(3):497-510, November 2023b. ISSN 2150-8097. doi: 10.14778/ 3632093.3632111. URL https://doi.org/10.14778/3632093.3632111 .\\\\n12\\\\n\", \"level\": 1}]', 'parsing_status': 'success', 'parsing_error': None, 'param_1': datetime.datetime(2026, 1, 25, 13, 45, 49, 263848)}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/2j85)\n",
      "\n",
      "âœ“ Enhanced pipeline test successful!\n",
      "âœ“ System continued processing despite PDF failures\n"
     ]
    }
   ],
   "source": [
    "# Test Complete Pipeline with PDF Processing\n",
    "print(\"Test 8: Complete Pipeline with PDF Processing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reuse metadata fetcher from Test 6\n",
    "print(\"âœ“ Using metadata fetcher service from previous test\")\n",
    "\n",
    "# Test with small batch including PDF processing\n",
    "print(\"Running enhanced test (3 papers with PDF processing)...\")\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        results = await metadata_fetcher.fetch_and_process_papers(\n",
    "            max_results=3,  # Small batch\n",
    "            from_date=\"20250813\",  # Recent date\n",
    "            to_date=\"20250814\",\n",
    "            process_pdfs=True,  \n",
    "            store_to_db=True,\n",
    "            db_session=session\n",
    "        )\n",
    "    \n",
    "    print(\"\\nENHANCED PIPELINE RESULTS:\")\n",
    "    print(f\"   Papers fetched: {results.get('papers_fetched', 0)}\")\n",
    "    print(f\"   PDFs downloaded: {results.get('pdfs_downloaded', 0)}\")\n",
    "    print(f\"   PDFs parsed: {results.get('pdfs_parsed', 0)}\")\n",
    "    print(f\"   Papers stored: {results.get('papers_stored', 0)}\")\n",
    "    print(f\"   Processing time: {results.get('processing_time', 0):.1f}s\")\n",
    "    print(f\"   Errors: {len(results.get('errors', []))}\")\n",
    "    \n",
    "    # Show success rates\n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        download_rate = (results['pdfs_downloaded'] / results['papers_fetched']) * 100\n",
    "        parse_rate = (results['pdfs_parsed'] / results['pdfs_downloaded']) * 100 if results.get('pdfs_downloaded', 0) > 0 else 0\n",
    "        print(f\"   Download success rate: {download_rate:.1f}%\")\n",
    "        print(f\"   Parse success rate: {parse_rate:.1f}%\")\n",
    "    \n",
    "    if results.get('errors'):\n",
    "        print(\"\\nErrors encountered (showing graceful error handling):\")\n",
    "        for error in results.get('errors', [])[:3]:  # Show first 3 errors\n",
    "            print(f\"   - {error}\")\n",
    "    \n",
    "    if results.get('papers_fetched', 0) > 0:\n",
    "        print(\"\\nâœ“ Enhanced pipeline test successful!\")\n",
    "        if results.get('errors'):\n",
    "            print(\"âœ“ System continued processing despite PDF failures\")\n",
    "    else:\n",
    "        print(\"\\n! No papers fetched - may be arXiv API unavailability\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 21:11:48,502 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-25 21:11:48,540 INFO sqlalchemy.engine.Engine SELECT papers.id AS papers_id, papers.arxiv_id AS papers_arxiv_id, papers.title AS papers_title, papers.authors AS papers_authors, papers.abstract AS papers_abstract, papers.categories AS papers_categories, papers.published_date AS papers_published_date, papers.updated_date AS papers_updated_date, papers.pdf_url AS papers_pdf_url, papers.pdf_content AS papers_pdf_content, papers.sections AS papers_sections, papers.parsing_status AS papers_parsing_status, papers.parsing_error AS papers_parsing_error, papers.created_at AS papers_created_at, papers.updated_at AS papers_updated_at \n",
      "FROM papers\n",
      "2026-01-25 21:11:48,541 INFO sqlalchemy.engine.Engine [generated in 0.00257s] {}\n",
      "2508.11121: Tabularis Formatus: Predictive Formatting for Tabl...\n",
      "2601.16210: PyraTok: Language-Aligned Pyramidal Tokenizer for ...\n",
      "2508.11112: Quantization through Piecewise-Affine Regularizati...\n",
      "2508.11110: Diffusion is a code repair operator and generator...\n",
      "2601.16211: Why Can't I Open My Drawer? Mitigating Object-Driv...\n",
      "2026-01-25 21:11:48,602 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "from src.db.factory import make_database                                                                                         \n",
    "                                                                                                                                   \n",
    "db = make_database()                                                                                                             \n",
    "with db.get_session() as session:                                                                                                \n",
    "  from src.models.paper import Paper                                                                                           \n",
    "  papers = session.query(Paper).all()                                                                                          \n",
    "  for p in papers:                                                                                                             \n",
    "      print(f\"{p.arxiv_id}: {p.title[:50]}...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT papers.id AS papers_id, papers.arxiv_id AS papers_arxiv_id, papers.title AS papers_title, papers.authors AS papers_authors, papers.abstract AS papers_abstract, papers.categories AS papers_categories, papers.published_date AS papers_published_date, papers.updated_date AS papers_updated_date, papers.pdf_url AS papers_pdf_url, papers.pdf_content AS papers_pdf_content, papers.sections AS papers_sections, papers.parsing_status AS papers_parsing_status, papers.parsing_error AS papers_parsing_error, papers.created_at AS papers_created_at, papers.updated_at AS papers_updated_at \n",
    "FROM papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
