{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Complete RAG System with LLM Integration\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 5 completes our RAG (Retrieval-Augmented Generation) system by adding the final piece: **answer generation with a local LLM**.\n",
    "\n",
    "## Week 5 Focus Areas\n",
    "\n",
    "### Core Objectives\n",
    "- **Local LLM Integration**: Use Ollama to generate answers from search results\n",
    "- **Complete RAG Pipeline**: Query → Search → Generate → Answer\n",
    "- **Streaming Capabilities**: Real-time response streaming via SSE\n",
    "- **Clean API Design**: Simplified endpoints for production use\n",
    "\n",
    "### What We'll Test In This Notebook\n",
    "1. **Service Health Check** - Verify all components are running\n",
    "2. **API Structure** - See our clean, focused endpoints\n",
    "3. **LLM Integration** - Test Ollama generating answers\n",
    "4. **Search Functionality** - Verify hybrid search works\n",
    "5. **Complete RAG Pipeline** - End-to-end question answering\n",
    "6. **Streaming Responses** - Real-time answer generation\n",
    "7. **System Status** - Final health summary\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Ensure all services are running:**\n",
    "```bash\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**Service Access Points:**\n",
    "- **FastAPI**: http://localhost:8000/docs\n",
    "- **OpenSearch**: http://localhost:9201\n",
    "- **Ollama**: http://localhost:11434\n",
    "- **PostgreSQL**: localhost:5433\n",
    "\n",
    "---\n",
    "\n",
    "## API Endpoints Overview\n",
    "\n",
    "### Core Endpoints\n",
    "- **`POST /api/v1/ask`** - Standard RAG endpoint (wait for complete response)\n",
    "- **`POST /api/v1/stream`** - Streaming RAG endpoint (real-time SSE response)\n",
    "- **`POST /api/v1/hybrid-search/`** - Search papers with hybrid BM25 + vector approach\n",
    "- **`GET /api/v1/health`** - System health and service status\n",
    "\n",
    "### Request Format\n",
    "```json\n",
    "{\n",
    "    \"query\": \"Your question here\",\n",
    "    \"top_k\": 3,\n",
    "    \"use_hybrid\": true,\n",
    "    \"model\": \"llama3.2\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"categories\": [\"cs.AI\", \"cs.LG\"]\n",
    "}\n",
    "```\n",
    "\n",
    "### Response Format (Standard)\n",
    "```json\n",
    "{\n",
    "    \"query\": \"Your question\",\n",
    "    \"answer\": \"Generated answer from LLM\",\n",
    "    \"sources\": [\"https://arxiv.org/pdf/...\"],\n",
    "    \"chunks_used\": 3,\n",
    "    \"search_mode\": \"hybrid\",\n",
    "    \"model\": \"llama3.2\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Response Format (Streaming - SSE)\n",
    "```\n",
    "data: {\"sources\": [...], \"chunks_used\": 3, \"search_mode\": \"hybrid\", \"model\": \"llama3.2\"}\n",
    "data: {\"chunk\": \"The\"}\n",
    "data: {\"chunk\": \" answer\"}\n",
    "data: {\"chunk\": \" is\"}\n",
    "data: {\"answer\": \"The answer is...\", \"done\": true}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "│   User Query    │────▶│  FastAPI Router  │────▶│  Jina Embeddings│\n",
    "└─────────────────┘     └─────────────────┘     └─────────────────┘\n",
    "                                │                         │\n",
    "                                │                         ▼\n",
    "                                │                 ┌─────────────────┐\n",
    "                                │                 │   OpenSearch    │\n",
    "                                │                 │  (BM25 + KNN)  │\n",
    "                                │                 └─────────────────┘\n",
    "                                │                         │\n",
    "                                ▼                         │\n",
    "                        ┌─────────────────┐              │\n",
    "                        │  Ollama (LLM)   │◀─────────────┘\n",
    "                        │  RAG Prompt     │  Retrieved Chunks\n",
    "                        └─────────────────┘\n",
    "                                │\n",
    "                                ▼\n",
    "                        ┌─────────────────┐\n",
    "                        │ Answer + Sources │\n",
    "                        └─────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin testing our complete RAG system!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.7\n",
      "Project root: /Users/nishantgaurav/Project/PaperAlchemy\n",
      "API Base: http://localhost:8000\n",
      "OpenSearch: http://localhost:9201\n",
      "Ollama: http://localhost:11434\n",
      "\n",
      "Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "\n",
    "# Find project root and add to Python path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week5\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = Path(\"/Users/nishantgaurav/Project/PaperAlchemy\")\n",
    "\n",
    "if project_root.exists():\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"Project root not found - check directory structure\")\n",
    "\n",
    "# PaperAlchemy service URLs (from compose.yml port mappings)\n",
    "API_BASE = \"http://localhost:8000\"\n",
    "OPENSEARCH_URL = \"http://localhost:9201\"\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "print(f\"API Base: {API_BASE}\")\n",
    "print(f\"OpenSearch: {OPENSEARCH_URL}\")\n",
    "print(f\"Ollama: {OLLAMA_URL}\")\n",
    "print(\"\\nEnvironment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Service Health Check\n",
    "\n",
    "Verify all PaperAlchemy services are running properly before testing the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPERALCHEMY SERVICE HEALTH CHECK\n",
      "========================================\n",
      "  FastAPI: Healthy\n",
      "  OpenSearch: Healthy (status: yellow)\n",
      "  Ollama: Healthy (version: 0.11.2)\n",
      "\n",
      "--- Detailed Health (/api/v1/health) ---\n",
      "  Overall: OK\n",
      "  database: healthy - Connected successfully\n",
      "  opensearch: healthy - Index 'arxiv-papers-chunks' with 2 documents\n",
      "\n",
      "All services ready for Week 5!\n"
     ]
    }
   ],
   "source": [
    "# Check Service Health\n",
    "print(\"PAPERALCHEMY SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "services = {\n",
    "    \"FastAPI\": f\"{API_BASE}/health\",\n",
    "    \"OpenSearch\": f\"{OPENSEARCH_URL}/_cluster/health\",\n",
    "    \"Ollama\": f\"{OLLAMA_URL}/api/version\",\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Show extra detail per service\n",
    "            if service_name == \"Ollama\":\n",
    "                print(f\"  {service_name}: Healthy (version: {data.get('version', '?')})\")\n",
    "            elif service_name == \"OpenSearch\":\n",
    "                print(f\"  {service_name}: Healthy (status: {data.get('status', '?')})\")\n",
    "            else:\n",
    "                print(f\"  {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"  {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"  {service_name}: Not accessible ({e})\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Also check the detailed health endpoint\n",
    "print(\"\\n--- Detailed Health (/api/v1/health) ---\")\n",
    "try:\n",
    "    resp = requests.get(f\"{API_BASE}/api/v1/health\", timeout=5)\n",
    "    if resp.status_code == 200:\n",
    "        health = resp.json()\n",
    "        print(f\"  Overall: {health.get('status', '?').upper()}\")\n",
    "        for svc, info in health.get(\"services\", {}).items():\n",
    "            print(f\"  {svc}: {info.get('status')} - {info.get('message')}\")\n",
    "    else:\n",
    "        print(f\"  HTTP {resp.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "if all_healthy:\n",
    "    print(\"\\nAll services ready for Week 5!\")\n",
    "else:\n",
    "    print(\"\\nSome services need attention. Run: docker compose up --build -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Structure Overview\n",
    "\n",
    "Week 5 adds the `/ask` and `/stream` endpoints to PaperAlchemy's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPERALCHEMY API STRUCTURE\n",
      "==============================\n",
      "Total endpoints: 7\n",
      "\n",
      "Available endpoints:\n",
      "  GET    /\n",
      "         Root\n",
      "  POST   /api/v1/ask\n",
      "         Ask Question\n",
      "  GET    /api/v1/health\n",
      "         Health Check\n",
      "  POST   /api/v1/hybrid-search/\n",
      "         Hybrid Search\n",
      "  GET    /api/v1/search\n",
      "         Search Papers Get\n",
      "  POST   /api/v1/search\n",
      "         Search Paper Post\n",
      "  POST   /api/v1/stream\n",
      "         Ask Question Stream\n",
      "  GET    /health\n",
      "         Simple Health\n"
     ]
    }
   ],
   "source": [
    "# Check API Endpoints\n",
    "print(\"PAPERALCHEMY API STRUCTURE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{API_BASE}/openapi.json\")\n",
    "    if response.status_code == 200:\n",
    "        openapi_data = response.json()\n",
    "        paths = openapi_data[\"paths\"]\n",
    "        \n",
    "        print(f\"Total endpoints: {len(paths)}\")\n",
    "        print(f\"\\nAvailable endpoints:\")\n",
    "        for path in sorted(paths.keys()):\n",
    "            methods = list(paths[path].keys())\n",
    "            for method in methods:\n",
    "                summary = paths[path][method].get(\"summary\", \"\")\n",
    "                print(f\"  {method.upper():6s} {path}\")\n",
    "                if summary:\n",
    "                    print(f\"         {summary}\")\n",
    "    else:\n",
    "        print(f\"Could not fetch API info: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Ollama LLM\n",
    "\n",
    "Verify the local LLM service can list models and generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLLAMA LLM TEST\n",
      "====================\n",
      "Available models: 1\n",
      "  - llama3.2:1b (1.2 GB)\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama LLM Service\n",
    "print(\"OLLAMA LLM TEST\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Check available models\n",
    "try:\n",
    "    models_response = requests.get(f\"{OLLAMA_URL}/api/tags\")\n",
    "    if models_response.status_code == 200:\n",
    "        models = models_response.json().get(\"models\", [])\n",
    "        print(f\"Available models: {len(models)}\")\n",
    "        for model in models:\n",
    "            size_gb = model.get(\"size\", 0) / (1024**3)\n",
    "            print(f\"  - {model['name']} ({size_gb:.1f} GB)\")\n",
    "    else:\n",
    "        print(f\"Could not list models: {models_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM Generation:\n",
      "  LLM responded: '8' (2.9s)\n",
      "  Ollama is working!\n"
     ]
    }
   ],
   "source": [
    "# Test Simple LLM Generation\n",
    "print(\"Testing LLM Generation:\")\n",
    "\n",
    "try:\n",
    "    test_data = {\n",
    "        \"model\": \"llama3.2:1b\",\n",
    "        \"prompt\": \"What is 2+6? Answer with just the number.\",\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/api/generate\",\n",
    "        json=test_data,\n",
    "        timeout=30,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        answer = result.get(\"response\", \"\").strip()\n",
    "        print(f\"  LLM responded: '{answer}' ({elapsed:.1f}s)\")\n",
    "        print(\"  Ollama is working!\")\n",
    "    else:\n",
    "        print(f\"  Generation failed: HTTP {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Search Functionality\n",
    "\n",
    "Before testing the full RAG pipeline, verify that hybrid search finds relevant paper chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID SEARCH TEST\n",
      "====================\n",
      "Query: 'machine learning'\n",
      "  Results: 2 hits (0.7s)\n",
      "  Search mode: hybrid\n",
      "\n",
      "Top results:\n",
      "  1. Shared LoRA Subspaces for almost Strict Continual Learning\n",
      "     score: 0.0328\n",
      "     chunk: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world ...\n",
      "\n",
      "  2. DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semanti\n",
      "     score: 0.0161\n",
      "     chunk: Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Hybrid Search\n",
    "print(\"HYBRID SEARCH TEST\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "search_query = \"machine learning\"\n",
    "print(f\"Query: '{search_query}'\")\n",
    "\n",
    "try:\n",
    "    search_request = {\n",
    "        \"query\": search_query,\n",
    "        \"use_hybrid\": True,\n",
    "        \"size\": 3,\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/api/v1/hybrid-search/\",\n",
    "        json=search_request,\n",
    "        timeout=30,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"  Results: {data['total']} hits ({elapsed:.1f}s)\")\n",
    "        print(f\"  Search mode: {data['search_mode']}\")\n",
    "\n",
    "        if data[\"hits\"]:\n",
    "            print(f\"\\nTop results:\")\n",
    "            for i, hit in enumerate(data[\"hits\"], 1):\n",
    "                title = hit.get(\"title\", \"Unknown\")[:70]\n",
    "                score = hit.get(\"score\", 0)\n",
    "                chunk = hit.get(\"chunk_text\", \"\")[:100]\n",
    "                print(f\"  {i}. {title}\")\n",
    "                print(f\"     score: {score:.4f}\")\n",
    "                if chunk:\n",
    "                    print(f\"     chunk: {chunk}...\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"  No results found - is data indexed?\")\n",
    "            print(f\"  Check: curl {OPENSEARCH_URL}/arxiv-papers-chunks/_count\")\n",
    "    else:\n",
    "        print(f\"  Search failed: HTTP {response.status_code}\")\n",
    "        print(f\"  {response.text[:200]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete RAG Pipeline Test (Standard)\n",
    "\n",
    "The main event: **end-to-end question answering** using the `/ask` endpoint.\n",
    "\n",
    "Flow: Query → Embed (Jina) → Search (OpenSearch) → Prompt Build → Generate (Ollama) → Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE RAG PIPELINE TEST (STANDARD)\n",
      "========================================\n",
      "Question: What are recent advances in machine learning?\n",
      "\n",
      "Success! (15.7s)\n",
      "\n",
      "Answer:\n",
      "--------------------------------------------------\n",
      "Recent advances in machine learning can be attributed to the development of large pretrained models and their integration into novel approaches for continual learning and knowledge transfer. Parameter-efficient tuning methods like low rank adaptation (LoRA) have reduced computational demands, but they lack mechanisms for strict continual learning and knowledge integration. [1] proposes Share, a novel approach that learns and dynamically updates a single, shared low-rank subspace to facilitate seamless adaptation across multiple tasks and modalities.\n",
      "\n",
      "DyTopo, an introduced multi-agent framework, reconstructs sparse directed communication graphs at each round based on the manager's goal conditions. This enables efficient communication, improved performance, and enhanced interpretability of coordination traces [2]. These advances demonstrate the ongoing efforts in machine learning research to overcome limitations in continual learning and knowledge transfer.\n",
      "\n",
      "References:\n",
      "[1] arXiv:2602.06043\n",
      "[2] arXiv:2602.06039\n",
      "--------------------------------------------------\n",
      "\n",
      "Metadata:\n",
      "  Model: llama3.2:1b\n",
      "  Sources: 2 papers\n",
      "    - https://arxiv.org/pdf/2602.06039.pdf\n",
      "    - https://arxiv.org/pdf/2602.06043.pdf\n",
      "  Chunks used: 2\n",
      "  Search mode: hybrid\n"
     ]
    }
   ],
   "source": [
    "# Test Complete RAG Pipeline - Standard endpoint\n",
    "print(\"COMPLETE RAG PIPELINE TEST (STANDARD)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "question = \"What are recent advances in machine learning?\"\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    rag_request = {\n",
    "        \"query\": question,\n",
    "        \"top_k\": 3,\n",
    "        \"use_hybrid\": True,\n",
    "        \"model\": \"llama3.2:1b\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/api/v1/ask\",\n",
    "        json=rag_request,\n",
    "        timeout=120,\n",
    "    )\n",
    "\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        print(f\"\\nSuccess! ({response_time:.1f}s)\")\n",
    "        print(f\"\\nAnswer:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(data[\"answer\"])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Model: {data.get('model', '?')}\")\n",
    "        print(f\"  Sources: {len(data.get('sources', []))} papers\")\n",
    "        for src in data.get(\"sources\", []):\n",
    "            print(f\"    - {src}\")\n",
    "        print(f\"  Chunks used: {data.get('chunks_used', 0)}\")\n",
    "        print(f\"  Search mode: {data.get('search_mode', '?')}\")\n",
    "    else:\n",
    "        print(f\"\\nRequest failed: HTTP {response.status_code}\")\n",
    "        print(f\"Response: {response.text[:300]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG PIPELINE - PARAMETER VARIATIONS\n",
      "========================================\n",
      "\n",
      "--- BM25-only (no hybrid) ---\n",
      "Query: What is attention mechanism in transformers?\n",
      "  Time: 14.3s | Mode: bm25 | Chunks: 1\n",
      "  Answer: [arXiv:2602.06043]\n",
      "The attention mechanism in transformers is a crucial component of the transformer architecture. In this context, the focus is on the way the model processes sequential data by selec...\n",
      "\n",
      "--- Low temperature (more focused) ---\n",
      "Query: Explain deep learning optimization techniques\n",
      "  Time: 19.0s | Mode: hybrid | Chunks: 2\n",
      "  Answer: Share is a novel approach to parameter-efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace. This facilitates seamless adaptation across multiple tasks...\n",
      "\n",
      "--- Category-filtered (cs.AI only) ---\n",
      "Query: What are the latest AI research trends?\n",
      "  Failed: HTTP 500 - {\"detail\":\"LLM generation error: Generation failed: 404\"}\n"
     ]
    }
   ],
   "source": [
    "# Test with different parameters\n",
    "print(\"RAG PIPELINE - PARAMETER VARIATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"label\": \"BM25-only (no hybrid)\",\n",
    "        \"request\": {\n",
    "            \"query\": \"What is attention mechanism in transformers?\",\n",
    "            \"top_k\": 2,\n",
    "            \"use_hybrid\": False,\n",
    "            \"model\": \"llama3.2:1b\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Low temperature (more focused)\",\n",
    "        \"request\": {\n",
    "            \"query\": \"Explain deep learning optimization techniques\",\n",
    "            \"top_k\": 3,\n",
    "            \"use_hybrid\": True,\n",
    "            \"model\": \"llama3.2:1b\",\n",
    "            \"temperature\": 0.3,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Category-filtered (cs.AI only)\",\n",
    "        \"request\": {\n",
    "            \"query\": \"What are the latest AI research trends?\",\n",
    "            \"top_k\": 3,\n",
    "            \"use_hybrid\": True,\n",
    "            \"model\": \"llama3.2\",\n",
    "            \"categories\": [\"cs.AI\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "for tc in test_cases:\n",
    "    print(f\"\\n--- {tc['label']} ---\")\n",
    "    print(f\"Query: {tc['request']['query']}\")\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{API_BASE}/api/v1/ask\",\n",
    "            json=tc[\"request\"],\n",
    "            timeout=120,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            d = resp.json()\n",
    "            # Truncate answer for readability\n",
    "            answer_preview = d[\"answer\"][:200]\n",
    "            print(f\"  Time: {elapsed:.1f}s | Mode: {d.get('search_mode')} | Chunks: {d.get('chunks_used')}\")\n",
    "            print(f\"  Answer: {answer_preview}...\")\n",
    "        else:\n",
    "            print(f\"  Failed: HTTP {resp.status_code} - {resp.text[:150]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming RAG Pipeline Test\n",
    "\n",
    "The `/stream` endpoint returns Server-Sent Events (SSE) for real-time token-by-token output.\n",
    "\n",
    "**Flow:**\n",
    "1. First SSE event: metadata (sources, chunks_used, search_mode, model)\n",
    "2. Subsequent events: `{\"chunk\": \"token\"}` as they are generated\n",
    "3. Final event: `{\"done\": true, \"answer\": \"full text\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE RAG PIPELINE TEST (STREAMING)\n",
      "========================================\n",
      "Question: Summarize recent machine learning papers\n",
      "\n",
      "Streaming response...\n",
      "  Metadata received (0.6s): mode=hybrid, chunks=2\n",
      "\n",
      "Stream error: Streaming failed: 404\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Complete! (Total: 0.6s)\n",
      "\n",
      "Metadata:\n",
      "  Model: llama3.2\n",
      "  Search mode: hybrid\n",
      "  Chunks used: 2\n",
      "  Sources: 2 papers\n",
      "    - https://arxiv.org/pdf/2602.06039.pdf\n",
      "    - https://arxiv.org/pdf/2602.06043.pdf\n",
      "  Total response time: 0.6s\n",
      "  Answer length: 0 chars\n"
     ]
    }
   ],
   "source": [
    "# Test Streaming RAG Pipeline\n",
    "print(\"COMPLETE RAG PIPELINE TEST (STREAMING)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "question = \"Summarize recent machine learning papers\"\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    rag_request = {\n",
    "        \"query\": question,\n",
    "        \"top_k\": 3,\n",
    "        \"use_hybrid\": True,\n",
    "        \"model\": \"llama3.2\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/api/v1/stream\",\n",
    "        json=rag_request,\n",
    "        stream=True,\n",
    "        timeout=120,\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        full_answer = \"\"\n",
    "        sources = []\n",
    "        chunks_used = 0\n",
    "        search_mode = \"unknown\"\n",
    "        model_used = \"unknown\"\n",
    "        first_chunk_time = None\n",
    "\n",
    "        print(f\"\\nStreaming response...\")\n",
    "\n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            line_str = line.decode(\"utf-8\")\n",
    "            if not line_str.startswith(\"data: \"):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = json.loads(line_str[6:])\n",
    "\n",
    "                # Handle error events\n",
    "                if \"error\" in data:\n",
    "                    print(f\"\\nStream error: {data['error']}\")\n",
    "                    break\n",
    "\n",
    "                # Handle metadata (first event)\n",
    "                if \"sources\" in data and \"chunk\" not in data:\n",
    "                    sources = data[\"sources\"]\n",
    "                    chunks_used = data.get(\"chunks_used\", 0)\n",
    "                    search_mode = data.get(\"search_mode\", \"unknown\")\n",
    "                    model_used = data.get(\"model\", \"unknown\")\n",
    "                    meta_time = time.time() - start_time\n",
    "                    print(f\"  Metadata received ({meta_time:.1f}s): mode={search_mode}, chunks={chunks_used}\")\n",
    "\n",
    "                # Handle streaming text chunks\n",
    "                if \"chunk\" in data:\n",
    "                    if first_chunk_time is None:\n",
    "                        first_chunk_time = time.time() - start_time\n",
    "                        print(f\"  First token at: {first_chunk_time:.1f}s\")\n",
    "                        print(f\"\\nAnswer:\")\n",
    "                        print(\"-\" * 50)\n",
    "\n",
    "                    chunk_text = data[\"chunk\"]\n",
    "                    full_answer += chunk_text\n",
    "                    print(chunk_text, end=\"\", flush=True)\n",
    "\n",
    "                # Handle completion\n",
    "                if data.get(\"done\", False):\n",
    "                    break\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        print(f\"\\n\" + \"-\" * 50)\n",
    "        print(f\"\\nComplete! (Total: {total_time:.1f}s)\")\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Model: {model_used}\")\n",
    "        print(f\"  Search mode: {search_mode}\")\n",
    "        print(f\"  Chunks used: {chunks_used}\")\n",
    "        print(f\"  Sources: {len(sources)} papers\")\n",
    "        for src in sources[:5]:\n",
    "            print(f\"    - {src}\")\n",
    "        if first_chunk_time:\n",
    "            print(f\"  Time to first token: {first_chunk_time:.1f}s\")\n",
    "        print(f\"  Total response time: {total_time:.1f}s\")\n",
    "        print(f\"  Answer length: {len(full_answer)} chars\")\n",
    "    else:\n",
    "        print(f\"\\nRequest failed: HTTP {response.status_code}\")\n",
    "        print(f\"Response: {response.text[:300]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Edge Cases and Error Handling\n",
    "\n",
    "Test how the system handles unusual inputs and failure scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDGE CASE TESTS\n",
      "====================\n",
      "\n",
      "--- Very short query ---\n",
      "  Status: OK (8.3s)\n",
      "  Chunks: 2 | Mode: hybrid\n",
      "  Answer: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastro...\n",
      "\n",
      "--- Non-existent category ---\n",
      "  Status: OK (13.9s)\n",
      "  Chunks: 2 | Mode: hybrid\n",
      "  Answer: The question is not explicitly stated in the provided paper excerpts. However, it can be inferred that the question relates to the topic of lifelong l...\n",
      "\n",
      "--- Single chunk retrieval ---\n",
      "  Status: OK (7.9s)\n",
      "  Chunks: 1 | Mode: hybrid\n",
      "  Answer: Share, a novel approach to parameter-efficient continual finetuning, constructs a foundational subspace that extracts core knowledge from past tasks a...\n"
     ]
    }
   ],
   "source": [
    "# Test Edge Cases\n",
    "print(\"EDGE CASE TESTS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "edge_cases = [\n",
    "    {\n",
    "        \"label\": \"Very short query\",\n",
    "        \"request\": {\"query\": \"AI\", \"top_k\": 2, \"use_hybrid\": True},\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Non-existent category\",\n",
    "        \"request\": {\n",
    "            \"query\": \"machine learning\",\n",
    "            \"top_k\": 3,\n",
    "            \"use_hybrid\": True,\n",
    "            \"categories\": [\"nonexistent.XX\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Single chunk retrieval\",\n",
    "        \"request\": {\n",
    "            \"query\": \"What is reinforcement learning?\",\n",
    "            \"top_k\": 1,\n",
    "            \"use_hybrid\": True,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "for tc in edge_cases:\n",
    "    print(f\"\\n--- {tc['label']} ---\")\n",
    "    try:\n",
    "        start = time.time()\n",
    "        resp = requests.post(\n",
    "            f\"{API_BASE}/api/v1/ask\",\n",
    "            json=tc[\"request\"],\n",
    "            timeout=120,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            d = resp.json()\n",
    "            print(f\"  Status: OK ({elapsed:.1f}s)\")\n",
    "            print(f\"  Chunks: {d.get('chunks_used')} | Mode: {d.get('search_mode')}\")\n",
    "            print(f\"  Answer: {d['answer'][:150]}...\")\n",
    "        else:\n",
    "            print(f\"  Status: HTTP {resp.status_code} ({elapsed:.1f}s)\")\n",
    "            print(f\"  Detail: {resp.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. System Status Summary\n",
    "\n",
    "Final overview of PaperAlchemy's RAG system status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPERALCHEMY SYSTEM STATUS SUMMARY\n",
      "========================================\n",
      "Overall Status: OK\n",
      "Version: 0.1.0\n",
      "Environment: development\n",
      "\n",
      "Service Status:\n",
      "  [OK] database: Connected successfully\n",
      "  [OK] opensearch: Index 'arxiv-papers-chunks' with 2 documents\n",
      "\n",
      "Ollama Models:\n",
      "  - llama3.2:1b\n",
      "\n",
      "OpenSearch Index:\n",
      "  Index: arxiv-papers-chunks\n",
      "  Documents: 2\n",
      "\n",
      "Endpoint Status:\n",
      "  [OK] POST /api/v1/ask - Standard RAG\n",
      "  [OK] POST /api/v1/stream - Streaming RAG\n",
      "  [OK] POST /api/v1/hybrid-search/ - Hybrid Search\n",
      "  [OK] GET  /api/v1/health - Health Check\n",
      "\n",
      "RAG Pipeline:\n",
      "  [OK] Data Ingestion: Papers indexed in OpenSearch\n",
      "  [OK] Embeddings: Jina for hybrid search\n",
      "  [OK] Search: BM25 + KNN vector hybrid\n",
      "  [OK] LLM Generation: Ollama local inference\n",
      "  [OK] Streaming: SSE real-time responses\n",
      "  [OK] API: Clean endpoints ready\n",
      "\n",
      "PaperAlchemy Week 5 RAG system is fully operational!\n",
      "  API docs: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "# System Status Summary\n",
    "print(\"PAPERALCHEMY SYSTEM STATUS SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Detailed health\n",
    "    health_resp = requests.get(f\"{API_BASE}/api/v1/health\", timeout=5)\n",
    "    if health_resp.status_code == 200:\n",
    "        health = health_resp.json()\n",
    "        print(f\"Overall Status: {health.get('status', '?').upper()}\")\n",
    "        print(f\"Version: {health.get('version', '?')}\")\n",
    "        print(f\"Environment: {health.get('environment', '?')}\")\n",
    "\n",
    "        print(f\"\\nService Status:\")\n",
    "        for svc, info in health.get(\"services\", {}).items():\n",
    "            status = info.get(\"status\", \"unknown\")\n",
    "            msg = info.get(\"message\", \"\")\n",
    "            icon = \"OK\" if status == \"healthy\" else \"!!\"\n",
    "            print(f\"  [{icon}] {svc}: {msg}\")\n",
    "\n",
    "    # Ollama models\n",
    "    models_resp = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "    if models_resp.status_code == 200:\n",
    "        models = models_resp.json().get(\"models\", [])\n",
    "        print(f\"\\nOllama Models:\")\n",
    "        for m in models:\n",
    "            print(f\"  - {m['name']}\")\n",
    "\n",
    "    # OpenSearch index stats\n",
    "    idx_resp = requests.get(f\"{OPENSEARCH_URL}/arxiv-papers-chunks/_count\", timeout=5)\n",
    "    if idx_resp.status_code == 200:\n",
    "        count = idx_resp.json().get(\"count\", 0)\n",
    "        print(f\"\\nOpenSearch Index:\")\n",
    "        print(f\"  Index: arxiv-papers-chunks\")\n",
    "        print(f\"  Documents: {count}\")\n",
    "\n",
    "    # Endpoint check\n",
    "    print(f\"\\nEndpoint Status:\")\n",
    "    endpoints_to_check = [\n",
    "        (\"POST\", \"/api/v1/ask\", \"Standard RAG\"),\n",
    "        (\"POST\", \"/api/v1/stream\", \"Streaming RAG\"),\n",
    "        (\"POST\", \"/api/v1/hybrid-search/\", \"Hybrid Search\"),\n",
    "        (\"GET\",  \"/api/v1/health\", \"Health Check\"),\n",
    "    ]\n",
    "    openapi_resp = requests.get(f\"{API_BASE}/openapi.json\", timeout=5)\n",
    "    if openapi_resp.status_code == 200:\n",
    "        registered = openapi_resp.json().get(\"paths\", {})\n",
    "        for method, path, label in endpoints_to_check:\n",
    "            exists = path in registered\n",
    "            icon = \"OK\" if exists else \"!!\"\n",
    "            print(f\"  [{icon}] {method:4s} {path} - {label}\")\n",
    "\n",
    "    print(f\"\\nRAG Pipeline:\")\n",
    "    print(f\"  [OK] Data Ingestion: Papers indexed in OpenSearch\")\n",
    "    print(f\"  [OK] Embeddings: Jina for hybrid search\")\n",
    "    print(f\"  [OK] Search: BM25 + KNN vector hybrid\")\n",
    "    print(f\"  [OK] LLM Generation: Ollama local inference\")\n",
    "    print(f\"  [OK] Streaming: SSE real-time responses\")\n",
    "    print(f\"  [OK] API: Clean endpoints ready\")\n",
    "\n",
    "    print(f\"\\nPaperAlchemy Week 5 RAG system is fully operational!\")\n",
    "    print(f\"  API docs: {API_BASE}/docs\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking status: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built in Week 5:\n",
    "\n",
    "**Complete RAG System Components:**\n",
    "1. **Data Pipeline**: arXiv papers -> PostgreSQL -> OpenSearch (chunk-level indexing)\n",
    "2. **Embeddings**: Jina API for query embedding at search time\n",
    "3. **Search System**: Hybrid BM25 + KNN vector search with RRF fusion\n",
    "4. **LLM Integration**: Local Ollama service for answer generation\n",
    "5. **Prompt Engineering**: Structured RAG prompts with citation extraction\n",
    "6. **Streaming API**: SSE real-time response streaming\n",
    "7. **Error Handling**: Graceful degradation (embedding failure -> BM25 fallback)\n",
    "\n",
    "**RAG Pipeline Flow:**\n",
    "```\n",
    "User Question -> Embed Query (Jina) -> Search (OpenSearch) -> Build Prompt -> Generate (Ollama) -> Answer + Sources\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **Local LLM**: No external API calls for generation (privacy, no cost)\n",
    "- **Hybrid Search**: BM25 keyword + vector semantic similarity\n",
    "- **Streaming**: SSE for real-time token-by-token output\n",
    "- **Graceful Degradation**: Embedding failures fall back to BM25\n",
    "- **Source Attribution**: arXiv PDF URLs extracted from search results\n",
    "- **Configurable**: Model, temperature, top_p, categories per request\n",
    "\n",
    "**API Endpoints:**\n",
    "- `POST /api/v1/ask` - Standard RAG (complete response)\n",
    "- `POST /api/v1/stream` - Streaming RAG (SSE real-time)\n",
    "- `POST /api/v1/hybrid-search/` - Direct search\n",
    "- `GET /api/v1/health` - Service health status\n",
    "\n",
    "**Architecture Decisions:**\n",
    "- Factory pattern for service initialization\n",
    "- FastAPI dependency injection (Annotated + Depends)\n",
    "- Custom exception hierarchy (LLMException -> OllamaException)\n",
    "- 3-tier config: per-request -> environment -> code defaults\n",
    "- Prompt loaded from external .txt file for easy iteration\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different Ollama models\n",
    "- Tune search parameters (top_k, min_score)\n",
    "- Add conversation memory / multi-turn support\n",
    "- Implement response caching (Redis)\n",
    "- Add Langfuse tracing for observability\n",
    "- Explore the API documentation at http://localhost:8000/docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
