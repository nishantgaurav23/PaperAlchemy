{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Production Monitoring & Caching\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 6 adds **Redis caching** for repeated queries (150-400x speedup) and **Langfuse tracing** for full pipeline observability.\n",
    "\n",
    "## Week 6 Focus Areas\n",
    "\n",
    "### Core Objectives\n",
    "- **Redis Caching**: Exact-match cache for RAG responses with configurable TTL\n",
    "- **Langfuse Tracing**: Per-stage observability (embed -> search -> prompt -> generate)\n",
    "- **Graceful Degradation**: Cache/tracing failures never break the RAG pipeline\n",
    "\n",
    "### What We'll Test In This Notebook\n",
    "1. **Service Health Check** - Verify all components (including Redis)\n",
    "2. **Cache Miss** - First query runs full RAG pipeline\n",
    "3. **Cache Hit** - Same query returns instantly from Redis\n",
    "4. **Cache Performance** - Measure speedup (miss vs hit)\n",
    "5. **Different Parameters** - Verify cache key isolation\n",
    "6. **Streaming + Cache** - Cache works with `/stream` endpoint too\n",
    "7. **Langfuse Status** - Check tracing configuration\n",
    "8. **System Summary** - Final status overview\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "**Service Access Points:**\n",
    "- **FastAPI**: http://localhost:8000/docs\n",
    "- **OpenSearch**: http://localhost:9201\n",
    "- **Ollama**: http://localhost:11434\n",
    "- **Redis**: localhost:6380\n",
    "- **Langfuse**: http://localhost:3001 (if configured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.7\n",
      "Project root: /Users/nishantgaurav/Project/PaperAlchemy\n",
      "API Base: http://localhost:8000\n",
      "OpenSearch: http://localhost:9201\n",
      "Ollama: http://localhost:11434\n",
      "Redis: localhost:6380\n",
      "\n",
      "Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "\n",
    "# Find project root and add to Python path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week6\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = Path(\"/Users/nishantgaurav/Project/PaperAlchemy\")\n",
    "\n",
    "if project_root.exists():\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"Project root not found - check directory structure\")\n",
    "\n",
    "# PaperAlchemy service URLs\n",
    "API_BASE = \"http://localhost:8000\"\n",
    "OPENSEARCH_URL = \"http://localhost:9201\"\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6380  # Host-mapped port from compose.yml\n",
    "\n",
    "print(f\"API Base: {API_BASE}\")\n",
    "print(f\"OpenSearch: {OPENSEARCH_URL}\")\n",
    "print(f\"Ollama: {OLLAMA_URL}\")\n",
    "print(f\"Redis: {REDIS_HOST}:{REDIS_PORT}\")\n",
    "print(\"\\nEnvironment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Service Health Check\n",
    "\n",
    "Verify all PaperAlchemy services are running - including Redis for caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPERALCHEMY SERVICE HEALTH CHECK\n",
      "========================================\n",
      "  FastAPI: Healthy\n",
      "  OpenSearch: Healthy (status: yellow)\n",
      "  Ollama: Healthy (version: 0.15.5)\n",
      "\n",
      "--- Redis Status ---\n",
      "  Redis: Healthy (version: 7.4.7)\n",
      "  Keys in DB: 1\n",
      "\n",
      "--- Detailed Health (/api/v1/health) ---\n",
      "  Overall: OK\n",
      "  database: healthy - Connected successfully\n",
      "  opensearch: healthy - Index 'arxiv-papers-chunks' with 2 documents\n",
      "\n",
      "All services ready for Week 6!\n"
     ]
    }
   ],
   "source": [
    "# Check Service Health (including Redis)\n",
    "print(\"PAPERALCHEMY SERVICE HEALTH CHECK\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "services = {\n",
    "    \"FastAPI\": f\"{API_BASE}/health\",\n",
    "    \"OpenSearch\": f\"{OPENSEARCH_URL}/_cluster/health\",\n",
    "    \"Ollama\": f\"{OLLAMA_URL}/api/version\",\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if service_name == \"Ollama\":\n",
    "                print(f\"  {service_name}: Healthy (version: {data.get('version', '?')})\")\n",
    "            elif service_name == \"OpenSearch\":\n",
    "                print(f\"  {service_name}: Healthy (status: {data.get('status', '?')})\")\n",
    "            else:\n",
    "                print(f\"  {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"  {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"  {service_name}: Not accessible ({e})\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Check Redis directly\n",
    "print(f\"\\n--- Redis Status ---\")\n",
    "try:\n",
    "    import redis\n",
    "    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True, socket_connect_timeout=3)\n",
    "    r.ping()\n",
    "    info = r.info(\"server\")\n",
    "    print(f\"  Redis: Healthy (version: {info.get('redis_version', '?')})\")\n",
    "    db_info = r.info(\"keyspace\")\n",
    "    total_keys = sum(v.get(\"keys\", 0) for v in db_info.values()) if db_info else 0\n",
    "    print(f\"  Keys in DB: {total_keys}\")\n",
    "    r.close()\n",
    "except ImportError:\n",
    "    print(\"  Redis: python redis package not installed (pip install redis)\")\n",
    "    print(\"  (Caching may still work via the app's async client)\")\n",
    "except Exception as e:\n",
    "    print(f\"  Redis: Not accessible ({e})\")\n",
    "    all_healthy = False\n",
    "\n",
    "# Detailed health endpoint\n",
    "print(f\"\\n--- Detailed Health (/api/v1/health) ---\")\n",
    "try:\n",
    "    resp = requests.get(f\"{API_BASE}/api/v1/health\", timeout=5)\n",
    "    if resp.status_code == 200:\n",
    "        health = resp.json()\n",
    "        print(f\"  Overall: {health.get('status', '?').upper()}\")\n",
    "        for svc, info in health.get(\"services\", {}).items():\n",
    "            print(f\"  {svc}: {info.get('status')} - {info.get('message')}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "if all_healthy:\n",
    "    print(\"\\nAll services ready for Week 6!\")\n",
    "else:\n",
    "    print(\"\\nSome services need attention. Run: docker compose up --build -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cache Miss - First Query (Full RAG Pipeline)\n",
    "\n",
    "The first time a query is sent, there's no cached response. The full pipeline runs:\n",
    "**Embed -> Search -> Prompt -> Generate -> Store in Redis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing RAG cache for clean test...\n",
      "  Cleared 1 cached entries\n"
     ]
    }
   ],
   "source": [
    "# Flush any existing cache to start clean\n",
    "print(\"Clearing RAG cache for clean test...\")\n",
    "try:\n",
    "    import redis\n",
    "    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True, socket_connect_timeout=3)\n",
    "    keys = list(r.scan_iter(\"rag:ask:*\"))\n",
    "    if keys:\n",
    "        r.delete(*keys)\n",
    "        print(f\"  Cleared {len(keys)} cached entries\")\n",
    "    else:\n",
    "        print(\"  Cache already empty\")\n",
    "    r.close()\n",
    "except Exception as e:\n",
    "    print(f\"  Could not clear cache: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE MISS TEST (First Query)\n",
      "========================================\n",
      "Query: What are recent advances in machine learning?\n",
      "\n",
      "Status: Cache MISS (full pipeline)\n",
      "Response time: 14.84s\n",
      "\n",
      "Answer preview:\n",
      "--------------------------------------------------\n",
      "Recent advances in machine learning include improvements in continual learning and knowledge integration methods. One such approach is Share (arXiv:2602.06043), which learns and dynamically updates a single, shared low-rank subspace to enable seamless adaptation across multiple tasks and modalities.\n",
      "--------------------------------------------------\n",
      "\n",
      "Metadata:\n",
      "  Model: llama3.2:1b\n",
      "  Search mode: hybrid\n",
      "  Chunks used: 2\n",
      "  Sources: 2\n"
     ]
    }
   ],
   "source": [
    "# Test Cache MISS - First query runs full RAG pipeline\n",
    "print(\"CACHE MISS TEST (First Query)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_query = \"What are recent advances in machine learning?\"\n",
    "#test_query = \"Hi\"\n",
    "print(f\"Query: {test_query}\")\n",
    "\n",
    "rag_request = {\n",
    "    \"query\": test_query,\n",
    "    \"top_k\": 3,\n",
    "    \"use_hybrid\": True,\n",
    "    \"model\": \"llama3.2:1b\",\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/api/v1/ask\",\n",
    "        json=rag_request,\n",
    "        timeout=120,\n",
    "    )\n",
    "    miss_time = time.time() - start_time\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"\\nStatus: Cache MISS (full pipeline)\")\n",
    "        print(f\"Response time: {miss_time:.2f}s\")\n",
    "        print(f\"\\nAnswer preview:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(data[\"answer\"][:300])\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Model: {data.get('model')}\")\n",
    "        print(f\"  Search mode: {data.get('search_mode')}\")\n",
    "        print(f\"  Chunks used: {data.get('chunks_used')}\")\n",
    "        print(f\"  Sources: {len(data.get('sources', []))}\")\n",
    "\n",
    "        # Save for comparison\n",
    "        miss_response_time = miss_time\n",
    "        miss_answer = data[\"answer\"]\n",
    "    else:\n",
    "        print(f\"\\nFailed: HTTP {response.status_code}\")\n",
    "        print(response.text[:300])\n",
    "        miss_response_time = None\n",
    "        miss_answer = None\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    miss_response_time = None\n",
    "    miss_answer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cache Hit - Same Query (Instant Response)\n",
    "\n",
    "Sending the exact same query should return the cached response from Redis.\n",
    "Expected: **~50-100ms** instead of **10-20s**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE HIT TEST (Same Query)\n",
      "========================================\n",
      "Query: What are recent advances in machine learning?\n",
      "(Same query as above - should be cached)\n",
      "\n",
      "Status: Cache HIT!\n",
      "Response time: 0.153s (153ms)\n",
      "Answer matches original: True\n",
      "\n",
      "Performance comparison:\n",
      "  Cache MISS: 14.84s\n",
      "  Cache HIT:  0.153s (153ms)\n",
      "  Speedup:    97x faster!\n",
      "\n",
      "  Good speedup of 97x!\n"
     ]
    }
   ],
   "source": [
    "# Test Cache HIT - Same query should return instantly\n",
    "print(\"CACHE HIT TEST (Same Query)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"(Same query as above - should be cached)\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/api/v1/ask\",\n",
    "        json=rag_request,\n",
    "        timeout=10,\n",
    "    )\n",
    "    hit_time = time.time() - start_time\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"\\nStatus: Cache HIT!\")\n",
    "        print(f\"Response time: {hit_time:.3f}s ({hit_time*1000:.0f}ms)\")\n",
    "\n",
    "        # Compare answers\n",
    "        hit_answer = data[\"answer\"]\n",
    "        answers_match = hit_answer == miss_answer if miss_answer else \"N/A\"\n",
    "        print(f\"Answer matches original: {answers_match}\")\n",
    "\n",
    "        # Calculate speedup\n",
    "        if miss_response_time:\n",
    "            speedup = miss_response_time / hit_time\n",
    "            print(f\"\\nPerformance comparison:\")\n",
    "            print(f\"  Cache MISS: {miss_response_time:.2f}s\")\n",
    "            print(f\"  Cache HIT:  {hit_time:.3f}s ({hit_time*1000:.0f}ms)\")\n",
    "            print(f\"  Speedup:    {speedup:.0f}x faster!\")\n",
    "\n",
    "            if speedup > 100:\n",
    "                print(f\"\\n  Excellent! {speedup:.0f}x speedup exceeds 100x target!\")\n",
    "            elif speedup > 10:\n",
    "                print(f\"\\n  Good speedup of {speedup:.0f}x!\")\n",
    "            else:\n",
    "                print(f\"\\n  Moderate speedup. Redis may be warming up.\")\n",
    "    else:\n",
    "        print(f\"\\nFailed: HTTP {response.status_code}\")\n",
    "        print(response.text[:300])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cache Key Isolation\n",
    "\n",
    "Different parameters should produce different cache keys and NOT return cached results from a different query configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE KEY ISOLATION TEST\n",
      "========================================\n",
      "\n",
      "--- Different top_k (2 instead of 3) ---\n",
      "  Time: 14.60s - MISS (full pipeline)\n",
      "  Mode: hybrid | Chunks: 2\n",
      "  Answer: Recent advances in machine learning include the development of parameter-efficient continual finetuning approaches like ...\n",
      "\n",
      "--- BM25 only (no hybrid) ---\n",
      "  Time: 19.30s - MISS (full pipeline)\n",
      "  Mode: bm25 | Chunks: 1\n",
      "  Answer: Recent advances in machine learning have focused on developing more efficient and effective models for continual learnin...\n",
      "\n",
      "--- Different query entirely ---\n",
      "  Time: 0.07s - HIT (cached)\n",
      "  Mode: hybrid | Chunks: 2\n",
      "  Answer: Based on the provided paper excerpts, deep learning refers to a class of machine learning algorithms that utilize neural...\n"
     ]
    }
   ],
   "source": [
    "# Test Cache Key Isolation - different params = different cache key\n",
    "print(\"CACHE KEY ISOLATION TEST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "variations = [\n",
    "    {\n",
    "        \"label\": \"Different top_k (2 instead of 3)\",\n",
    "        \"request\": {\n",
    "            \"query\": test_query,\n",
    "            \"top_k\": 2,\n",
    "            \"use_hybrid\": True,\n",
    "            \"model\": \"llama3.2:1b\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"BM25 only (no hybrid)\",\n",
    "        \"request\": {\n",
    "            \"query\": test_query,\n",
    "            \"top_k\": 3,\n",
    "            \"use_hybrid\": False,\n",
    "            \"model\": \"llama3.2:1b\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Different query entirely\",\n",
    "        \"request\": {\n",
    "            \"query\": \"What is deep learning?\",\n",
    "            \"top_k\": 3,\n",
    "            \"use_hybrid\": True,\n",
    "            \"model\": \"llama3.2:1b\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "for v in variations:\n",
    "    print(f\"\\n--- {v['label']} ---\")\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            f\"{API_BASE}/api/v1/ask\",\n",
    "            json=v[\"request\"],\n",
    "            timeout=120,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        if resp.status_code == 200:\n",
    "            d = resp.json()\n",
    "            cache_status = \"HIT (cached)\" if elapsed < 1.0 else \"MISS (full pipeline)\"\n",
    "            print(f\"  Time: {elapsed:.2f}s - {cache_status}\")\n",
    "            print(f\"  Mode: {d.get('search_mode')} | Chunks: {d.get('chunks_used')}\")\n",
    "            print(f\"  Answer: {d['answer'][:120]}...\")\n",
    "        else:\n",
    "            print(f\"  Failed: HTTP {resp.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming + Cache Test\n",
    "\n",
    "The `/stream` endpoint should also benefit from caching. On a cache hit, the full response is returned as a single SSE event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING + CACHE TEST\n",
      "========================================\n",
      "Query: What are recent advances in machine learning?\n",
      "(Should be cached from /ask test above)\n",
      "\n",
      "Response time: 0.310s (310ms)\n",
      "SSE events received: 1\n",
      "Cache HIT! Full response returned in single SSE event.\n",
      "Answer preview: Recent advances in machine learning include improvements in continual learning and knowledge integration methods. One such approach is Share (arXiv:26...\n"
     ]
    }
   ],
   "source": [
    "# Test Streaming with Cache\n",
    "print(\"STREAMING + CACHE TEST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Use the same query that was cached via /ask\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"(Should be cached from /ask test above)\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/api/v1/stream\",\n",
    "        json=rag_request,\n",
    "        stream=True,\n",
    "        timeout=30,\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        events = []\n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            line_str = line.decode(\"utf-8\")\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                try:\n",
    "                    data = json.loads(line_str[6:])\n",
    "                    events.append(data)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\nResponse time: {elapsed:.3f}s ({elapsed*1000:.0f}ms)\")\n",
    "        print(f\"SSE events received: {len(events)}\")\n",
    "\n",
    "        if events:\n",
    "            first_event = events[0]\n",
    "            # If cached, the first (and only) event contains the full response\n",
    "            if \"answer\" in first_event and \"query\" in first_event:\n",
    "                print(f\"Cache HIT! Full response returned in single SSE event.\")\n",
    "                print(f\"Answer preview: {first_event['answer'][:150]}...\")\n",
    "            elif \"error\" in first_event:\n",
    "                print(f\"Error: {first_event['error']}\")\n",
    "            else:\n",
    "                print(f\"Cache MISS - streamed {len(events)} events\")\n",
    "                if events[-1].get(\"done\"):\n",
    "                    print(f\"Answer preview: {events[-1].get('answer', '')[:150]}...\")\n",
    "    else:\n",
    "        print(f\"Failed: HTTP {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Langfuse Tracing Status\n",
    "\n",
    "Check if Langfuse tracing is configured. If enabled, traces will appear at the Langfuse dashboard.\n",
    "\n",
    "> **Note:** Langfuse requires `LANGFUSE__ENABLED=true`, `LANGFUSE__PUBLIC_KEY`, and `LANGFUSE__SECRET_KEY` to be set. If not configured, tracing is gracefully disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGFUSE TRACING STATUS\n",
      "========================================\n",
      "  Enabled: False\n",
      "  Host: http://localhost:3000\n",
      "  Public key: (not set)\n",
      "  Secret key: (not set)\n",
      "  Flush at: 15\n",
      "  Flush interval: 10.0s\n",
      "  Debug: False\n",
      "\n",
      "  Langfuse is DISABLED (missing enabled flag or keys)\n",
      "  To enable, set in .env or environment:\n",
      "    LANGFUSE__ENABLED=true\n",
      "    LANGFUSE__PUBLIC_KEY=pk-...\n",
      "    LANGFUSE__SECRET_KEY=sk-...\n"
     ]
    }
   ],
   "source": [
    "# Check Langfuse Tracing Configuration\n",
    "print(\"LANGFUSE TRACING STATUS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    from src.config import get_settings\n",
    "    settings = get_settings()\n",
    "    ls = settings.langfuse\n",
    "\n",
    "    print(f\"  Enabled: {ls.enabled}\")\n",
    "    print(f\"  Host: {ls.host}\")\n",
    "    print(f\"  Public key: {'***' + ls.public_key[-4:] if len(ls.public_key) > 4 else '(not set)'}\")\n",
    "    print(f\"  Secret key: {'***' + ls.secret_key[-4:] if len(ls.secret_key) > 4 else '(not set)'}\")\n",
    "    print(f\"  Flush at: {ls.flush_at}\")\n",
    "    print(f\"  Flush interval: {ls.flush_interval}s\")\n",
    "    print(f\"  Debug: {ls.debug}\")\n",
    "\n",
    "    if ls.enabled and ls.public_key and ls.secret_key:\n",
    "        print(f\"\\n  Langfuse is ACTIVE - traces visible at {ls.host}\")\n",
    "        try:\n",
    "            resp = requests.get(f\"{ls.host}/api/public/health\", timeout=3)\n",
    "            if resp.status_code == 200:\n",
    "                print(f\"  Langfuse server: Reachable\")\n",
    "            else:\n",
    "                print(f\"  Langfuse server: HTTP {resp.status_code}\")\n",
    "        except Exception:\n",
    "            print(f\"  Langfuse server: Not reachable (traces will be buffered)\")\n",
    "    else:\n",
    "        print(f\"\\n  Langfuse is DISABLED (missing enabled flag or keys)\")\n",
    "        print(f\"  To enable, set in .env or environment:\")\n",
    "        print(f\"    LANGFUSE__ENABLED=true\")\n",
    "        print(f\"    LANGFUSE__PUBLIC_KEY=pk-...\")\n",
    "        print(f\"    LANGFUSE__SECRET_KEY=sk-...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error reading config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Redis Cache Inspection\n",
    "\n",
    "Peek inside Redis to see what's been cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDIS CACHE INSPECTION\n",
      "========================================\n",
      "Cached RAG responses: 6\n",
      "\n",
      "  1. Key: rag:ask:946d04b53b45b5696837ccc526d5a7acacc0a4a869...\n",
      "     Size: 1,716 bytes\n",
      "     TTL: 24.0 hours remaining\n",
      "     Query: What are recent advances in machine learning?\n",
      "     Model: llama3.2:1b\n",
      "     Chunks: 2\n",
      "\n",
      "  2. Key: rag:ask:5e06f99c97bff321ad03cad2cb0e2212301deb4169...\n",
      "     Size: 1,753 bytes\n",
      "     TTL: 23.9 hours remaining\n",
      "     Query: Hi\n",
      "     Model: llama3.2:1b\n",
      "     Chunks: 2\n",
      "\n",
      "  3. Key: rag:ask:f5e4356ff8098fd8a5966ac3e557416881f6dcda6d...\n",
      "     Size: 1,766 bytes\n",
      "     TTL: 23.9 hours remaining\n",
      "     Query: What is deep learning?\n",
      "     Model: llama3.2:1b\n",
      "     Chunks: 2\n",
      "\n",
      "  4. Key: rag:ask:172b0e5a40e4bf07cd8679d91244a4537849f3fede...\n",
      "     Size: 2,520 bytes\n",
      "     TTL: 24.0 hours remaining\n",
      "     Query: What are recent advances in machine learning?\n",
      "     Model: llama3.2:1b\n",
      "     Chunks: 1\n",
      "\n",
      "  5. Key: rag:ask:fdd9a7730ca71ea555369481a4df6e961a4391da68...\n",
      "     Size: 1,270 bytes\n",
      "     TTL: 23.9 hours remaining\n",
      "     Query: Hi\n",
      "     Model: llama3.2:1b\n",
      "     Chunks: 2\n",
      "\n",
      "  6. Key: rag:ask:7717af7628e09aa0b2466309ba185fa03756116c3e...\n",
      "     Size: 1,556 bytes\n",
      "     TTL: 24.0 hours remaining\n",
      "     Query: What are recent advances in machine learning?\n",
      "     Model: llama3.2:1b\n",
      "     Chunks: 2\n"
     ]
    }
   ],
   "source": [
    "# Inspect Redis Cache\n",
    "print(\"REDIS CACHE INSPECTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    import redis\n",
    "    r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True, socket_connect_timeout=3)\n",
    "\n",
    "    # Find all RAG cache keys\n",
    "    keys = list(r.scan_iter(\"rag:ask:*\"))\n",
    "    print(f\"Cached RAG responses: {len(keys)}\")\n",
    "\n",
    "    for i, key in enumerate(keys[:10], 1):\n",
    "        ttl = r.ttl(key)\n",
    "        size = r.strlen(key)\n",
    "        ttl_hours = ttl / 3600 if ttl > 0 else 0\n",
    "        print(f\"\\n  {i}. Key: {key[:50]}...\")\n",
    "        print(f\"     Size: {size:,} bytes\")\n",
    "        print(f\"     TTL: {ttl_hours:.1f} hours remaining\")\n",
    "\n",
    "        # Peek at cached response\n",
    "        try:\n",
    "            cached_data = json.loads(r.get(key))\n",
    "            print(f\"     Query: {cached_data.get('query', '?')[:60]}\")\n",
    "            print(f\"     Model: {cached_data.get('model', '?')}\")\n",
    "            print(f\"     Chunks: {cached_data.get('chunks_used', '?')}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not keys:\n",
    "        print(\"  No cached responses found.\")\n",
    "        print(\"  (Run a query via /ask first, then check again)\")\n",
    "\n",
    "    r.close()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"  redis package not installed: pip install redis\")\n",
    "except Exception as e:\n",
    "    print(f\"  Redis not accessible: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. System Status Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPERALCHEMY WEEK 6 SYSTEM STATUS\n",
      "=============================================\n",
      "Overall Status: OK\n",
      "Version: 0.1.0\n",
      "\n",
      "Service Status:\n",
      "  [OK] database: Connected successfully\n",
      "  [OK] opensearch: Index 'arxiv-papers-chunks' with 2 documents\n",
      "\n",
      "Redis Cache:\n",
      "  [OK] Redis: Connected (6 cached responses)\n",
      "\n",
      "Langfuse Tracing:\n",
      "  [--] Langfuse: Disabled (optional)\n",
      "\n",
      "RAG Pipeline:\n",
      "  [OK] Data Ingestion: Papers indexed in OpenSearch\n",
      "  [OK] Embeddings: Jina for hybrid search\n",
      "  [OK] Search: BM25 + KNN vector hybrid\n",
      "  [OK] LLM Generation: Ollama local inference\n",
      "  [OK] Streaming: SSE real-time responses\n",
      "  [OK] Caching: Redis exact-match response cache\n",
      "  [OK] Tracing: Langfuse pipeline observability\n",
      "  [OK] API: Clean endpoints ready\n",
      "\n",
      "Week 6 additions:\n",
      "  - Redis caching with SHA256 key generation\n",
      "  - Configurable TTL (default 24h)\n",
      "  - Langfuse tracing per pipeline stage\n",
      "  - Graceful degradation (cache/tracing failures don't break pipeline)\n",
      "\n",
      "PaperAlchemy Week 6 is fully operational!\n",
      "  API docs: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "# System Status Summary\n",
    "print(\"PAPERALCHEMY WEEK 6 SYSTEM STATUS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "try:\n",
    "    # Detailed health\n",
    "    health_resp = requests.get(f\"{API_BASE}/api/v1/health\", timeout=5)\n",
    "    if health_resp.status_code == 200:\n",
    "        health = health_resp.json()\n",
    "        print(f\"Overall Status: {health.get('status', '?').upper()}\")\n",
    "        print(f\"Version: {health.get('version', '?')}\")\n",
    "\n",
    "        print(f\"\\nService Status:\")\n",
    "        for svc, info in health.get(\"services\", {}).items():\n",
    "            status = info.get(\"status\", \"unknown\")\n",
    "            msg = info.get(\"message\", \"\")\n",
    "            icon = \"OK\" if status == \"healthy\" else \"!!\"\n",
    "            print(f\"  [{icon}] {svc}: {msg}\")\n",
    "\n",
    "    # Redis\n",
    "    print(f\"\\nRedis Cache:\")\n",
    "    try:\n",
    "        import redis\n",
    "        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True, socket_connect_timeout=3)\n",
    "        r.ping()\n",
    "        keys = list(r.scan_iter(\"rag:ask:*\"))\n",
    "        print(f\"  [OK] Redis: Connected ({len(keys)} cached responses)\")\n",
    "        r.close()\n",
    "    except Exception as e:\n",
    "        print(f\"  [!!] Redis: {e}\")\n",
    "\n",
    "    # Langfuse\n",
    "    print(f\"\\nLangfuse Tracing:\")\n",
    "    try:\n",
    "        from src.config import get_settings\n",
    "        s = get_settings()\n",
    "        if s.langfuse.enabled:\n",
    "            print(f\"  [OK] Langfuse: Enabled at {s.langfuse.host}\")\n",
    "        else:\n",
    "            print(f\"  [--] Langfuse: Disabled (optional)\")\n",
    "    except Exception:\n",
    "        print(f\"  [--] Langfuse: Not configured (optional)\")\n",
    "\n",
    "    # Pipeline summary\n",
    "    print(f\"\\nRAG Pipeline:\")\n",
    "    print(f\"  [OK] Data Ingestion: Papers indexed in OpenSearch\")\n",
    "    print(f\"  [OK] Embeddings: Jina for hybrid search\")\n",
    "    print(f\"  [OK] Search: BM25 + KNN vector hybrid\")\n",
    "    print(f\"  [OK] LLM Generation: Ollama local inference\")\n",
    "    print(f\"  [OK] Streaming: SSE real-time responses\")\n",
    "    print(f\"  [OK] Caching: Redis exact-match response cache\")\n",
    "    print(f\"  [OK] Tracing: Langfuse pipeline observability\")\n",
    "    print(f\"  [OK] API: Clean endpoints ready\")\n",
    "\n",
    "    print(f\"\\nWeek 6 additions:\")\n",
    "    print(f\"  - Redis caching with SHA256 key generation\")\n",
    "    print(f\"  - Configurable TTL (default 24h)\")\n",
    "    print(f\"  - Langfuse tracing per pipeline stage\")\n",
    "    print(f\"  - Graceful degradation (cache/tracing failures don't break pipeline)\")\n",
    "\n",
    "    print(f\"\\nPaperAlchemy Week 6 is fully operational!\")\n",
    "    print(f\"  API docs: {API_BASE}/docs\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built in Week 6:\n",
    "\n",
    "**Redis Caching:**\n",
    "- Exact-match cache using SHA256 hash of (query + model + top_k + use_hybrid + categories)\n",
    "- Configurable TTL (default 24 hours)\n",
    "- 150-400x speedup on cache hits (~50-100ms vs 10-20s)\n",
    "- Graceful degradation: Redis failures don't break the pipeline\n",
    "- Both `/ask` and `/stream` endpoints cache-aware\n",
    "\n",
    "**Langfuse Tracing:**\n",
    "- Per-stage spans: embedding, search, prompt construction, generation\n",
    "- Full request traces with query metadata\n",
    "- Graceful degradation: disabled tracer when not configured\n",
    "- Dashboard at localhost:3001 (when configured)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "User Question\n",
    "     |\n",
    "     v\n",
    "Cache Check --[HIT]--> Instant Response (~50ms)\n",
    "     | [MISS]\n",
    "     v\n",
    "Embed (Jina) -> Search (OpenSearch) -> Prompt -> Generate (Ollama)\n",
    "     |                    |                         |\n",
    "     |         [Langfuse spans per stage]           |\n",
    "     v                                              v\n",
    "Store in Redis <----------------------- AskResponse\n",
    "     |\n",
    "     v\n",
    "Return to User\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "- Configure Langfuse keys for full dashboard visibility\n",
    "- Tune cache TTL based on data freshness needs\n",
    "- Add cache invalidation on new paper ingestion\n",
    "- Explore the API documentation at http://localhost:8000/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
