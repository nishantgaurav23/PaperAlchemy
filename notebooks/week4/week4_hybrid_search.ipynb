{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 4: Document Chunking & Hybrid Search\n",
    "\n",
    "**What We're Building This Week:**\n",
    "\n",
    "Week 4 transforms our BM25 keyword search (Week 3) into a hybrid search system that combines keyword matching with semantic vector similarity. This means \"deep learning optimization\" now also finds papers about \"neural network training\" — even when those exact words don't appear.\n",
    "\n",
    "## Week 4 Focus Areas\n",
    "\n",
    "### Core Objectives\n",
    "- **Section-Based Chunking**: Split papers into ~600-word overlapping segments that respect document structure\n",
    "- **Vector Embeddings**: Generate 1024-dim vectors via Jina AI for semantic similarity\n",
    "- **Hybrid Search**: Combine BM25 + KNN vector search using Reciprocal Rank Fusion (RRF)\n",
    "- **Production API**: Test the `/api/v1/hybrid-search/` endpoint end-to-end\n",
    "\n",
    "### What We'll Test In This Notebook\n",
    "1. **Environment & Service Health** — Verify all Week 1-3 services + Jina API\n",
    "2. **Text Chunking** — Test TextChunker with real papers from PostgreSQL\n",
    "3. **Embedding Generation** — Generate real embeddings via Jina AI\n",
    "4. **Hybrid Indexing Pipeline** — Chunk → Embed → Index into OpenSearch\n",
    "5. **Search Modes** — Compare BM25 vs Vector vs Hybrid search\n",
    "6. **Production API** — Test the hybrid search endpoint\n",
    "7. **Performance Comparison** — Measure latency and result quality\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architecture\n",
    "```\n",
    "Paper (PostgreSQL) → TextChunker → Chunks → Jina API → Embeddings\n",
    "                                                          ↓\n",
    "                                          OpenSearch (chunk + embedding)\n",
    "                                                          ↓\n",
    "                          Query → BM25 + KNN → RRF Pipeline → Results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Services Required\n",
    "```bash\n",
    "docker compose up --build -d\n",
    "```\n",
    "\n",
    "### Jina AI API Key\n",
    "1. Sign up at https://jina.ai/embeddings/ (free tier: 1M tokens/month)\n",
    "2. Add to `.env`: `JINA_API_KEY=jina_your_key_here`\n",
    "\n",
    "**Note**: Without a Jina API key, embedding generation will fail. BM25 search still works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Path Configuration\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "\n",
    "# Find project root and add to Python path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week4\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"Missing compose.yml - check directory\")\n",
    "\n",
    "# Load environment variables from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "print(f\"\\nJina API key configured: {'Yes' if os.getenv('JINA_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Health Verification\n",
    "print(\"WEEK 4 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/health\",\n",
    "    \"OpenSearch\": \"http://localhost:9201\",\n",
    "    \"PostgreSQL (via API)\": \"http://localhost:8000/api/v1/health\",\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"  {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"  {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"  {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Check Jina API key\n",
    "jina_key = os.getenv(\"JINA_API_KEY\", \"\")\n",
    "if jina_key and jina_key != \"jina_your_key_here\":\n",
    "    print(f\"  Jina API Key: Configured\")\n",
    "else:\n",
    "    print(f\"  Jina API Key: NOT configured (embedding tests will fail)\")\n",
    "    all_healthy = False\n",
    "\n",
    "if all_healthy:\n",
    "    print(f\"\\nAll services healthy! Ready for Week 4.\")\n",
    "else:\n",
    "    print(f\"\\nSome services need attention. Check above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Fetch Sample Papers from PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get papers with parsed PDF content for chunking\n",
    "from src.db.factory import make_database\n",
    "from src.models.paper import Paper\n",
    "\n",
    "print(\"FETCHING PAPERS WITH PDF CONTENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "database = make_database()\n",
    "\n",
    "with database.get_session() as session:\n",
    "    # Get papers that have parsed PDF text\n",
    "    papers_with_text = session.query(Paper).filter(\n",
    "        Paper.pdf_content != None,\n",
    "        Paper.pdf_content != \"\"\n",
    "    ).all()\n",
    "\n",
    "    # Also get all papers (some may only have abstracts)\n",
    "    all_papers = session.query(Paper).all()\n",
    "\n",
    "    print(f\"Total papers in database: {len(all_papers)}\")\n",
    "    print(f\"Papers with PDF content: {len(papers_with_text)}\")\n",
    "\n",
    "    # Prepare paper data dicts for the indexing pipeline\n",
    "    sample_papers = []\n",
    "    for paper in all_papers:\n",
    "        text = paper.pdf_content or paper.abstract or \"\"\n",
    "        sample_papers.append({\n",
    "            \"id\": paper.id,\n",
    "            \"arxiv_id\": paper.arxiv_id,\n",
    "            \"title\": paper.title,\n",
    "            \"abstract\": paper.abstract or \"\",\n",
    "            \"raw_text\": text,\n",
    "            \"sections\": paper.sections,\n",
    "            \"authors\": paper.authors,\n",
    "            \"categories\": paper.categories,\n",
    "            \"published_date\": paper.published_date.isoformat() if paper.published_date else None,\n",
    "        })\n",
    "\n",
    "    print(f\"\\nPapers prepared for indexing:\")\n",
    "    for p in sample_papers:\n",
    "        text_len = len(p[\"raw_text\"])\n",
    "        has_sections = \"Yes\" if p[\"sections\"] else \"No\"\n",
    "        print(f\"  [{p['arxiv_id']}] {p['title'][:55]}...\")\n",
    "        print(f\"    Text: {text_len:,} chars | Sections: {has_sections}\")\n",
    "\n",
    "    test_paper = sample_papers[0] if sample_papers else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Test TextChunker — Section-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the TextChunker with a real paper\n",
    "from src.services.indexing.text_chunker import TextChunker\n",
    "from src.config import get_settings\n",
    "\n",
    "settings = get_settings()\n",
    "\n",
    "print(\"TEXT CHUNKER TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create chunker from settings\n",
    "chunker = TextChunker(\n",
    "    chunk_size=settings.chunking.chunk_size,\n",
    "    overlap_size=settings.chunking.overlap_size,\n",
    "    min_chunk_size=settings.chunking.min_chunk_size,\n",
    ")\n",
    "\n",
    "print(f\"Chunk size: {settings.chunking.chunk_size} words\")\n",
    "print(f\"Overlap: {settings.chunking.overlap_size} words\")\n",
    "print(f\"Min chunk: {settings.chunking.min_chunk_size} words\")\n",
    "\n",
    "if test_paper:\n",
    "    # Chunk the first paper\n",
    "    chunks = chunker.chunk_paper(\n",
    "        title=test_paper[\"title\"],\n",
    "        abstract=test_paper[\"abstract\"],\n",
    "        full_text=test_paper[\"raw_text\"],\n",
    "        arxiv_id=test_paper[\"arxiv_id\"],\n",
    "        paper_id=str(test_paper[\"id\"]),\n",
    "        sections=test_paper.get(\"sections\"),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPaper: {test_paper['arxiv_id']}\")\n",
    "    print(f\"Original text: {len(test_paper['raw_text'].split()):,} words\")\n",
    "    print(f\"Chunks created: {len(chunks)}\")\n",
    "\n",
    "    if chunks:\n",
    "        avg_words = sum(c.metadata.word_count for c in chunks) / len(chunks)\n",
    "        print(f\"Average chunk size: {avg_words:.0f} words\")\n",
    "\n",
    "        print(f\"\\nSample chunks:\")\n",
    "        for i, chunk in enumerate(chunks[:3]):\n",
    "            print(f\"\\n  Chunk {i}: section={chunk.metadata.section_title}\")\n",
    "            print(f\"    Words: {chunk.metadata.word_count}\")\n",
    "            print(f\"    Overlap prev/next: {chunk.metadata.overlap_with_previous}/{chunk.metadata.overlap_with_next}\")\n",
    "            print(f\"    Preview: {chunk.text[:120]}...\")\n",
    "else:\n",
    "    print(\"\\nNo papers available. Run Week 2 notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 3.1 Compare Overlap Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different overlap sizes\n",
    "print(\"OVERLAP STRATEGY COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if test_paper:\n",
    "    for overlap in [0, 50, 100, 150]:\n",
    "        test_chunker = TextChunker(chunk_size=600, overlap_size=overlap, min_chunk_size=100)\n",
    "        test_chunks = test_chunker.chunk_text(\n",
    "            text=test_paper[\"raw_text\"],\n",
    "            arxiv_id=test_paper[\"arxiv_id\"],\n",
    "            paper_id=str(test_paper[\"id\"]),\n",
    "        )\n",
    "        avg = sum(c.metadata.word_count for c in test_chunks) / len(test_chunks) if test_chunks else 0\n",
    "        print(f\"  Overlap {overlap:3d} words: {len(test_chunks):3d} chunks, avg {avg:.0f} words/chunk\")\n",
    "\n",
    "    print(f\"\\nRecommendation: 100-word overlap — good context preservation, minimal redundancy.\")\n",
    "else:\n",
    "    print(\"No test paper available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Test Jina Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the JinaEmbeddingsClient directly\n",
    "from src.services.embeddings.jina_client import JinaEmbeddingsClient\n",
    "\n",
    "print(\"JINA EMBEDDING GENERATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "jina_api_key = os.getenv(\"JINA_API_KEY\", \"\")\n",
    "\n",
    "if not jina_api_key:\n",
    "    print(\"No JINA_API_KEY set. Skipping embedding test.\")\n",
    "    print(\"Add JINA_API_KEY to your .env file.\")\n",
    "else:\n",
    "    client = JinaEmbeddingsClient(api_key=jina_api_key)\n",
    "\n",
    "    # Test passage embedding (document-side)\n",
    "    test_texts = [\n",
    "        \"Transformers use self-attention mechanisms for sequence modeling.\",\n",
    "        \"Gradient descent is an optimization algorithm for training neural networks.\",\n",
    "        \"The cat sat on the mat.\",  # Unrelated text for contrast\n",
    "    ]\n",
    "\n",
    "    print(\"Testing passage embeddings (retrieval.passage)...\")\n",
    "    passage_embeddings = await client.embed_passages(test_texts)\n",
    "\n",
    "    print(f\"  Generated {len(passage_embeddings)} embeddings\")\n",
    "    print(f\"  Dimension: {len(passage_embeddings[0])}\")\n",
    "\n",
    "    for i, emb in enumerate(passage_embeddings):\n",
    "        norm = sum(x * x for x in emb) ** 0.5\n",
    "        print(f\"  Text {i+1}: [{emb[0]:.4f}, {emb[1]:.4f}, ...] norm={norm:.3f}\")\n",
    "\n",
    "    # Test query embedding (query-side, asymmetric)\n",
    "    print(f\"\\nTesting query embedding (retrieval.query)...\")\n",
    "    query_emb = await client.embed_query(\"attention mechanism in transformers\")\n",
    "    print(f\"  Dimension: {len(query_emb)}\")\n",
    "    print(f\"  Preview: [{query_emb[0]:.4f}, {query_emb[1]:.4f}, ...]\")\n",
    "\n",
    "    # Compute cosine similarity to show asymmetric encoding works\n",
    "    import math\n",
    "    def cosine_sim(a, b):\n",
    "        dot = sum(x * y for x, y in zip(a, b))\n",
    "        norm_a = math.sqrt(sum(x * x for x in a))\n",
    "        norm_b = math.sqrt(sum(x * x for x in b))\n",
    "        return dot / (norm_a * norm_b) if norm_a and norm_b else 0.0\n",
    "\n",
    "    print(f\"\\nCosine similarity (query vs passages):\")\n",
    "    for i, emb in enumerate(passage_embeddings):\n",
    "        sim = cosine_sim(query_emb, emb)\n",
    "        print(f\"  '{test_texts[i][:50]}...' -> {sim:.4f}\")\n",
    "\n",
    "    await client.close()\n",
    "    print(f\"\\nEmbedding test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. OpenSearch Setup — Hybrid Index & RRF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenSearch client and hybrid index\n",
    "from src.services.opensearch.factory import make_opensearch_client_fresh\n",
    "\n",
    "print(\"OPENSEARCH HYBRID INDEX SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create client pointing to localhost (notebook port)\n",
    "opensearch_client = make_opensearch_client_fresh(\n",
    "    settings=settings,\n",
    "    host=\"http://localhost:9201\"\n",
    ")\n",
    "\n",
    "print(f\"Host: {opensearch_client.host}\")\n",
    "print(f\"Index: {opensearch_client.index_name}\")\n",
    "print(f\"Health: {'Healthy' if opensearch_client.health_check() else 'Unhealthy'}\")\n",
    "\n",
    "# Setup hybrid index + RRF pipeline (creates if not exists)\n",
    "results = opensearch_client.setup_indices(force=False)\n",
    "print(f\"\\nIndex created: {results.get('hybrid_index', False)}\")\n",
    "print(f\"RRF pipeline created: {results.get('rrf_pipeline', False)}\")\n",
    "\n",
    "# Show current stats\n",
    "stats = opensearch_client.get_index_stats()\n",
    "print(f\"\\nCurrent index stats:\")\n",
    "print(f\"  Documents: {stats.get('document_count', 0)}\")\n",
    "print(f\"  Size: {stats.get('size_in_bytes', 0):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Hybrid Indexing Pipeline — Chunk → Embed → Index\n",
    "\n",
    "This is the core Week 4 pipeline. For each paper:\n",
    "1. TextChunker splits it into ~600-word overlapping chunks\n",
    "2. Jina API embeds each chunk into a 1024-dim vector\n",
    "3. OpenSearch stores chunk text + embedding + paper metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full hybrid indexing pipeline\n",
    "from src.services.indexing.factory import make_hybrid_indexing_service\n",
    "\n",
    "print(\"HYBRID INDEXING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not jina_api_key:\n",
    "    print(\"No JINA_API_KEY. Cannot run indexing pipeline.\")\n",
    "else:\n",
    "    # Create the fully-wired indexing service\n",
    "    indexing_service = make_hybrid_indexing_service(\n",
    "        settings=settings,\n",
    "        opensearch_host=\"http://localhost:9201\"\n",
    "    )\n",
    "\n",
    "    # Delete existing chunks first (clean slate)\n",
    "    print(\"Clearing existing chunks...\")\n",
    "    for paper in sample_papers:\n",
    "        opensearch_client.delete_paper_chunks(paper[\"arxiv_id\"])\n",
    "\n",
    "    # Index all papers\n",
    "    print(f\"\\nIndexing {len(sample_papers)} papers...\\n\")\n",
    "\n",
    "    total_stats = await indexing_service.index_papers_batch(\n",
    "        papers=sample_papers,\n",
    "        replace_existing=False,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPipeline Results:\")\n",
    "    print(f\"  Papers processed: {total_stats['papers_processed']}\")\n",
    "    print(f\"  Chunks created: {total_stats['total_chunks_created']}\")\n",
    "    print(f\"  Chunks indexed: {total_stats['total_chunks_indexed']}\")\n",
    "    print(f\"  Embeddings generated: {total_stats['total_embeddings_generated']}\")\n",
    "    print(f\"  Errors: {total_stats['total_errors']}\")\n",
    "\n",
    "    # Verify in OpenSearch\n",
    "    import time\n",
    "    time.sleep(1)  # Wait for refresh\n",
    "    stats = opensearch_client.get_index_stats()\n",
    "    print(f\"\\nOpenSearch index now has {stats.get('document_count', 0)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect indexed chunks for one paper\n",
    "print(\"INSPECT INDEXED CHUNKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if test_paper:\n",
    "    indexed_chunks = opensearch_client.get_chunks_by_paper(test_paper[\"arxiv_id\"])\n",
    "    print(f\"Paper: {test_paper['arxiv_id']}\")\n",
    "    print(f\"Chunks in OpenSearch: {len(indexed_chunks)}\")\n",
    "\n",
    "    for chunk in indexed_chunks[:3]:\n",
    "        print(f\"\\n  Chunk {chunk.get('chunk_index', '?')}:\")\n",
    "        print(f\"    Section: {chunk.get('section_title', 'N/A')}\")\n",
    "        print(f\"    Words: {chunk.get('chunk_word_count', 0)}\")\n",
    "        print(f\"    Model: {chunk.get('embedding_model', 'N/A')}\")\n",
    "        text_preview = chunk.get('chunk_text', '')[:120]\n",
    "        print(f\"    Text: {text_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Search Mode Comparison — BM25 vs Vector vs Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BM25 keyword search\n",
    "print(\"MODE 1: BM25 KEYWORD SEARCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_queries = [\"machine learning\", \"neural network\", \"optimization\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = opensearch_client.search_papers(query=query, size=3)\n",
    "    total = results.get(\"total\", 0)\n",
    "    print(f\"\\n  Query: '{query}' -> {total} results\")\n",
    "    for hit in results.get(\"hits\", [])[:2]:\n",
    "        print(f\"    [{hit.get('score', 0):.2f}] {hit.get('title', 'N/A')[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector similarity search\n",
    "print(\"MODE 2: VECTOR SIMILARITY SEARCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not jina_api_key:\n",
    "    print(\"No JINA_API_KEY. Skipping vector search.\")\n",
    "else:\n",
    "    embed_client = JinaEmbeddingsClient(api_key=jina_api_key)\n",
    "\n",
    "    for query in test_queries:\n",
    "        query_vec = await embed_client.embed_query(query)\n",
    "        results = opensearch_client.search_chunks_vectors(\n",
    "            query_embedding=query_vec, size=3\n",
    "        )\n",
    "        total = results.get(\"total\", 0)\n",
    "        print(f\"\\n  Query: '{query}' -> {total} results\")\n",
    "        for hit in results.get(\"hits\", [])[:2]:\n",
    "            print(f\"    [{hit.get('score', 0):.4f}] {hit.get('title', 'N/A')[:60]}...\")\n",
    "\n",
    "    await embed_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search (BM25 + Vector with RRF)\n",
    "print(\"MODE 3: HYBRID SEARCH (BM25 + VECTOR + RRF)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not jina_api_key:\n",
    "    print(\"No JINA_API_KEY. Skipping hybrid search.\")\n",
    "else:\n",
    "    embed_client = JinaEmbeddingsClient(api_key=jina_api_key)\n",
    "\n",
    "    for query in test_queries:\n",
    "        query_vec = await embed_client.embed_query(query)\n",
    "        results = opensearch_client.search_unified(\n",
    "            query=query,\n",
    "            query_embedding=query_vec,\n",
    "            size=3,\n",
    "            use_hybrid=True,\n",
    "        )\n",
    "        total = results.get(\"total\", 0)\n",
    "        print(f\"\\n  Query: '{query}' -> {total} results (RRF fused)\")\n",
    "        for hit in results.get(\"hits\", [])[:2]:\n",
    "            section = hit.get(\"section_title\", \"N/A\")\n",
    "            print(f\"    [{hit.get('score', 0):.4f}] {hit.get('title', 'N/A')[:50]}... | section: {section}\")\n",
    "\n",
    "    await embed_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Test Production API Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the /api/v1/hybrid-search/ endpoint\n",
    "import requests\n",
    "\n",
    "API_BASE = \"http://localhost:8000/api/v1\"\n",
    "\n",
    "print(\"PRODUCTION API ENDPOINT TESTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: BM25-only via hybrid endpoint (use_hybrid=False)\n",
    "print(\"\\n--- Test 1: BM25-Only Search ---\")\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/hybrid-search/\",\n",
    "        json={\"query\": \"neural network\", \"use_hybrid\": False, \"size\": 3},\n",
    "        timeout=10,\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"  Search mode: {data['search_mode']}\")\n",
    "        print(f\"  Total results: {data['total']}\")\n",
    "        for hit in data[\"hits\"][:2]:\n",
    "            print(f\"    [{hit['score']:.2f}] {hit['title'][:55]}...\")\n",
    "    else:\n",
    "        print(f\"  Failed: HTTP {response.status_code} - {response.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "# Test 2: Hybrid search (use_hybrid=True)\n",
    "print(\"\\n--- Test 2: Hybrid Search (BM25 + Vector) ---\")\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/hybrid-search/\",\n",
    "        json={\"query\": \"deep learning optimization\", \"use_hybrid\": True, \"size\": 3},\n",
    "        timeout=30,  # Longer timeout — includes Jina API call\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"  Search mode: {data['search_mode']}\")\n",
    "        print(f\"  Total results: {data['total']}\")\n",
    "        for hit in data[\"hits\"][:2]:\n",
    "            chunk_preview = (hit.get(\"chunk_text\") or \"\")[:80]\n",
    "            print(f\"    [{hit['score']:.4f}] {hit['title'][:55]}...\")\n",
    "            if chunk_preview:\n",
    "                print(f\"      Chunk: {chunk_preview}...\")\n",
    "    else:\n",
    "        print(f\"  Failed: HTTP {response.status_code} - {response.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "# Test 3: With category filter\n",
    "print(\"\\n--- Test 3: Hybrid Search with Category Filter ---\")\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/hybrid-search/\",\n",
    "        json={\n",
    "            \"query\": \"transformer attention\",\n",
    "            \"use_hybrid\": True,\n",
    "            \"size\": 3,\n",
    "            \"categories\": [\"cs.AI\", \"cs.LG\"],\n",
    "        },\n",
    "        timeout=30,\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"  Search mode: {data['search_mode']}\")\n",
    "        print(f\"  Total results: {data['total']}\")\n",
    "        for hit in data[\"hits\"][:2]:\n",
    "            print(f\"    [{hit['score']:.4f}] {hit['title'][:55]}...\")\n",
    "    else:\n",
    "        print(f\"  Failed: HTTP {response.status_code} - {response.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "print(f\"\\nSwagger UI: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison across all search modes\n",
    "import time\n",
    "\n",
    "print(\"SEARCH PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "query = \"machine learning artificial intelligence\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "results_table = []\n",
    "\n",
    "# BM25 via client\n",
    "start = time.time()\n",
    "try:\n",
    "    bm25_res = opensearch_client.search_papers(query=query, size=5)\n",
    "    bm25_time = time.time() - start\n",
    "    results_table.append((\"Client BM25\", bm25_time, bm25_res.get(\"total\", 0)))\n",
    "except Exception as e:\n",
    "    results_table.append((\"Client BM25\", 0, f\"Error: {e}\"))\n",
    "\n",
    "# BM25 via API\n",
    "start = time.time()\n",
    "try:\n",
    "    r = requests.post(f\"{API_BASE}/hybrid-search/\", json={\"query\": query, \"use_hybrid\": False, \"size\": 5}, timeout=10)\n",
    "    api_bm25_time = time.time() - start\n",
    "    results_table.append((\"API BM25\", api_bm25_time, r.json()[\"total\"] if r.status_code == 200 else \"Error\"))\n",
    "except Exception as e:\n",
    "    results_table.append((\"API BM25\", 0, f\"Error: {e}\"))\n",
    "\n",
    "# Hybrid via API\n",
    "start = time.time()\n",
    "try:\n",
    "    r = requests.post(f\"{API_BASE}/hybrid-search/\", json={\"query\": query, \"use_hybrid\": True, \"size\": 5}, timeout=30)\n",
    "    api_hybrid_time = time.time() - start\n",
    "    if r.status_code == 200:\n",
    "        d = r.json()\n",
    "        results_table.append((f\"API Hybrid ({d['search_mode']})\", api_hybrid_time, d[\"total\"]))\n",
    "    else:\n",
    "        results_table.append((\"API Hybrid\", api_hybrid_time, f\"HTTP {r.status_code}\"))\n",
    "except Exception as e:\n",
    "    results_table.append((\"API Hybrid\", 0, f\"Error: {e}\"))\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Method':<25} {'Time (s)':<12} {'Results'}\")\n",
    "print(\"-\" * 50)\n",
    "for method, t, count in results_table:\n",
    "    print(f\"{method:<25} {t:<12.3f} {count}\")\n",
    "\n",
    "print(f\"\\nNotes:\")\n",
    "print(f\"  - BM25 is fast (~50ms) — pure keyword matching in OpenSearch\")\n",
    "print(f\"  - Hybrid includes Jina API latency (~1-3s) for query embedding\")\n",
    "print(f\"  - Hybrid provides semantic matching that BM25 cannot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. Graceful Degradation Test\n",
    "\n",
    "Verify that hybrid search falls back to BM25 when embeddings fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search_unified fallback behavior\n",
    "print(\"GRACEFUL DEGRADATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Case 1: No embedding provided -> should use BM25\n",
    "print(\"\\n1. No embedding (query_embedding=None):\")\n",
    "results = opensearch_client.search_unified(\n",
    "    query=\"neural network\",\n",
    "    query_embedding=None,\n",
    "    use_hybrid=True,\n",
    "    size=3,\n",
    ")\n",
    "print(f\"   Results: {results.get('total', 0)} (should use BM25 fallback)\")\n",
    "\n",
    "# Case 2: use_hybrid=False -> should use BM25\n",
    "print(\"\\n2. Hybrid disabled (use_hybrid=False):\")\n",
    "results = opensearch_client.search_unified(\n",
    "    query=\"neural network\",\n",
    "    query_embedding=[0.1] * 1024,  # Dummy embedding\n",
    "    use_hybrid=False,\n",
    "    size=3,\n",
    ")\n",
    "print(f\"   Results: {results.get('total', 0)} (should use BM25)\")\n",
    "\n",
    "# Case 3: Both provided -> should use hybrid\n",
    "if jina_api_key:\n",
    "    print(\"\\n3. Full hybrid (embedding + use_hybrid=True):\")\n",
    "    embed_client = JinaEmbeddingsClient(api_key=jina_api_key)\n",
    "    query_vec = await embed_client.embed_query(\"neural network\")\n",
    "    results = opensearch_client.search_unified(\n",
    "        query=\"neural network\",\n",
    "        query_embedding=query_vec,\n",
    "        use_hybrid=True,\n",
    "        size=3,\n",
    "    )\n",
    "    print(f\"   Results: {results.get('total', 0)} (should use hybrid RRF)\")\n",
    "    await embed_client.close()\n",
    "\n",
    "print(f\"\\nDegradation test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built & Tested\n",
    "\n",
    "1. **TextChunker** — Section-aware chunking with 600-word target, 100-word overlap\n",
    "2. **Jina Embeddings** — 1024-dim vectors with asymmetric encoding (passage vs query)\n",
    "3. **Hybrid Indexing Pipeline** — Chunk → Embed → Bulk index into OpenSearch\n",
    "4. **Three Search Modes**:\n",
    "   - **BM25**: Fast keyword matching (~50ms)\n",
    "   - **Vector**: Semantic similarity via KNN\n",
    "   - **Hybrid**: BM25 + Vector fused with RRF pipeline\n",
    "5. **Production API** — `/api/v1/hybrid-search/` with graceful degradation\n",
    "6. **Graceful Degradation** — Falls back to BM25 when embeddings fail\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Paper → TextChunker → Chunks → Jina API → Embeddings → OpenSearch\n",
    "                                                            ↓\n",
    "                      Query → Embed → BM25 + KNN → RRF → Results\n",
    "```\n",
    "\n",
    "### Next Steps (Week 5)\n",
    "- LLM integration (Ollama) for answer generation from search results\n",
    "- Complete RAG pipeline: Query → Search → Context → Generate → Response\n",
    "- Conversation memory and context management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}