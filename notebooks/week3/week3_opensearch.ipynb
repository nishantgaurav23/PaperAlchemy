{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 3: Keyword Search First - The Critical Foundation\n",
    "\n",
    "> **The 90% Problem:** Most RAG systems jump straight to vector search and miss the foundation that powers the best retrieval systems. We're doing it right!\n",
    "\n",
    "## What We're Building This Week\n",
    "\n",
    "Week 3 focuses on implementing OpenSearch integration for full-text search capabilities using BM25 scoring. This transforms our system from a simple storage solution into a searchable knowledge base.\n",
    "\n",
    "### Core Objectives\n",
    "- **OpenSearch Integration**: Connect our FastAPI application to OpenSearch cluster\n",
    "- **Index Management**: Create and manage the arxiv-papers index with proper mappings\n",
    "- **BM25 Search**: Implement full-text search with relevance scoring\n",
    "- **Data Pipeline**: Transfer papers from PostgreSQL to OpenSearch\n",
    "- **Search API**: Expose search functionality through REST endpoints\n",
    "\n",
    "### What We'll Test In This Notebook\n",
    "1. **Infrastructure Verification** - Ensure all services from Week 1-2 are running\n",
    "2. **OpenSearch Service Integration** - Test client creation and health checks\n",
    "3. **Index Creation & Management** - Create arxiv-papers index with proper mappings\n",
    "4. **Data Pipeline** - Transfer papers from PostgreSQL to OpenSearch\n",
    "5. **BM25 Search Functionality** - Test search queries with relevance scoring\n",
    "6. **Search API Endpoints** - Verify FastAPI search endpoints work correctly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.12.7\n",
      "Environment: /Users/nishantgaurav/Project/PaperAlchemy/.venv/bin/python\n",
      "Project root: /Users/nishantgaurav/Project/PaperAlchemy\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Path Configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "print(f\"Python Version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Environment: {sys.executable}\")\n",
    "\n",
    "# Find project root and add to Python path\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"week3\" and current_dir.parent.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "elif (current_dir / \"compose.yml\").exists():\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = None\n",
    "\n",
    "if project_root and (project_root / \"compose.yml\").exists():\n",
    "    print(f\"Project root: {project_root}\")\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "else:\n",
    "    print(\"Missing compose.yml - check directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Infrastructure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 3 PREREQUISITE CHECK\n",
      "==================================================\n",
      "✓ FastAPI: Healthy\n",
      "✓ OpenSearch: Healthy\n",
      "✓ Ollama: Healthy\n",
      "✓ Airflow: Healthy\n",
      "\n",
      "Checking PostgreSQL...\n",
      "2026-01-29 23:52:53,920 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2026-01-29 23:52:53,920 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2026-01-29 23:52:53,933 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2026-01-29 23:52:53,933 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2026-01-29 23:52:53,964 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2026-01-29 23:52:53,965 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2026-01-29 23:52:53,973 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-29 23:52:53,980 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2026-01-29 23:52:53,983 INFO sqlalchemy.engine.Engine [generated in 0.00260s] {'table_name': 'papers', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2026-01-29 23:52:53,999 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2026-01-29 23:52:54,004 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-29 23:52:54,005 INFO sqlalchemy.engine.Engine SELECT 1\n",
      "2026-01-29 23:52:54,005 INFO sqlalchemy.engine.Engine [generated in 0.00133s] {}\n",
      "2026-01-29 23:52:54,019 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "✓ PostgreSQL: Healthy\n",
      "\n",
      "All services healthy! Ready for Week 3 OpenSearch integration.\n"
     ]
    }
   ],
   "source": [
    "# Service Health Verification\n",
    "print(\"WEEK 3 PREREQUISITE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "services_to_test = {\n",
    "    \"FastAPI\": \"http://localhost:8000/health\",\n",
    "    \"OpenSearch\": \"http://localhost:9201\",\n",
    "    \"Ollama\": \"http://localhost:11434/api/version\",\n",
    "    \"Airflow\": \"http://localhost:8080/health\",\n",
    "}\n",
    "\n",
    "all_healthy = True\n",
    "\n",
    "for service_name, url in services_to_test.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ {service_name}: Healthy\")\n",
    "        else:\n",
    "            print(f\"✗ {service_name}: HTTP {response.status_code}\")\n",
    "            all_healthy = False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"✗ {service_name}: Not accessible\")\n",
    "        all_healthy = False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {service_name}: {type(e).__name__}\")\n",
    "        all_healthy = False\n",
    "\n",
    "# Test PostgreSQL directly\n",
    "print(\"\\nChecking PostgreSQL...\")\n",
    "try:\n",
    "    from src.db.factory import make_database\n",
    "    db = make_database()\n",
    "    if db.health_check():\n",
    "        print(\"✓ PostgreSQL: Healthy\")\n",
    "    else:\n",
    "        print(\"✗ PostgreSQL: Not accessible\")\n",
    "        all_healthy = False\n",
    "except Exception as e:\n",
    "    print(f\"✗ PostgreSQL: {e}\")\n",
    "    all_healthy = False\n",
    "\n",
    "print()\n",
    "if all_healthy:\n",
    "    print(\"All services healthy! Ready for Week 3 OpenSearch integration.\")\n",
    "else:\n",
    "    print(\"Some services need attention. Please run: docker compose up --build -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. OpenSearch Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENSEARCH CLIENT SETUP\n",
      "========================================\n",
      "Client configured with host: http://localhost:9201\n",
      "Index name: arxiv-papers-chunks\n",
      "✓ OpenSearch health check: PASSED\n",
      "   Cluster: docker-cluster\n",
      "   Status: yellow\n",
      "   Nodes: 1\n"
     ]
    }
   ],
   "source": [
    "# OpenSearch Client Setup\n",
    "from src.services.opensearch.factory import make_opensearch_client_fresh\n",
    "from src.config import get_settings\n",
    "\n",
    "print(\"OPENSEARCH CLIENT SETUP\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "settings = get_settings()\n",
    "\n",
    "# Create fresh OpenSearch client (localhost for notebook)\n",
    "opensearch_client = make_opensearch_client_fresh(\n",
    "    settings=settings,\n",
    "    host=\"http://localhost:9201\"  # PaperAlchemy OpenSearch port\n",
    ")\n",
    "\n",
    "print(f\"Client configured with host: {opensearch_client.host}\")\n",
    "print(f\"Index name: {opensearch_client.index_name}\")\n",
    "\n",
    "# Test health check\n",
    "is_healthy = opensearch_client.health_check()\n",
    "if is_healthy:\n",
    "    print(\"✓ OpenSearch health check: PASSED\")\n",
    "    \n",
    "    # Show cluster info\n",
    "    cluster_health = opensearch_client.client.cluster.health()\n",
    "    print(f\"   Cluster: {cluster_health['cluster_name']}\")\n",
    "    print(f\"   Status: {cluster_health['status']}\")\n",
    "    print(f\"   Nodes: {cluster_health['number_of_nodes']}\")\n",
    "else:\n",
    "    print(\"✗ OpenSearch health check: FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Index Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CONFIGURATION\n",
      "========================================\n",
      "Index Name: arxiv-papers-chunks\n",
      "\n",
      "Key Features:\n",
      "• Custom text analyzers for better search\n",
      "• Multi-field mapping (text + keyword)\n",
      "• Strict dynamic mapping\n",
      "\n",
      "Field Types:\n",
      "  • chunk_id: keyword\n",
      "  • arxiv_id: keyword\n",
      "  • paper_id: keyword\n",
      "  • chunk_index: integer\n",
      "  • chunk_text: text [text_analyzer]\n",
      "  • chunk_word_count: integer\n",
      "  • start_char: integer\n",
      "  • end_char: integer\n",
      "  • embedding: knn_vector\n",
      "  • title: text [text_analyzer]\n",
      "  • authors: text [standard_analyzer]\n",
      "  • abstract: text [text_analyzer]\n",
      "  • categories: keyword\n",
      "  • published_date: date\n",
      "  • section_title: keyword\n",
      "  • embedding_model: keyword\n",
      "  • created_at: date\n",
      "  • updated_at: date\n"
     ]
    }
   ],
   "source": [
    "# Display Index Configuration\n",
    "from src.services.opensearch.index_config import ARXIV_PAPERS_INDEX, ARXIV_PAPERS_CHUNKS_MAPPING\n",
    "\n",
    "print(\"INDEX CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Index Name: {opensearch_client.index_name}\")\n",
    "print(f\"\\nKey Features:\")\n",
    "print(\"• Custom text analyzers for better search\")\n",
    "print(\"• Multi-field mapping (text + keyword)\")\n",
    "print(\"• Strict dynamic mapping\")\n",
    "print(\"\\nField Types:\")\n",
    "\n",
    "properties = ARXIV_PAPERS_CHUNKS_MAPPING[\"mappings\"][\"properties\"]\n",
    "for field_name, config in properties.items():\n",
    "    field_type = config.get(\"type\")\n",
    "    analyzer = config.get(\"analyzer\", \"\")\n",
    "    if analyzer:\n",
    "        print(f\"  • {field_name}: {field_type} [{analyzer}]\")\n",
    "    else:\n",
    "        print(f\"  • {field_name}: {field_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CREATION\n",
      "========================================\n",
      "✓ Index 'arxiv-papers-chunks' already exists\n",
      "✓ RRF search pipeline created\n",
      "\n",
      "Current Statistics:\n",
      "   Documents: 5\n",
      "   Size: 144,911 bytes\n"
     ]
    }
   ],
   "source": [
    "# Create Index\n",
    "print(\"INDEX CREATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Setup indices (creates hybrid index + RRF pipeline)\n",
    "    results = opensearch_client.setup_indices(force=False)\n",
    "    \n",
    "    if results.get(\"hybrid_index\"):\n",
    "        print(f\"✓ Index '{opensearch_client.index_name}' created successfully!\")\n",
    "    else:\n",
    "        print(f\"✓ Index '{opensearch_client.index_name}' already exists\")\n",
    "    \n",
    "    if results.get(\"rrf_pipeline\"):\n",
    "        print(\"✓ RRF search pipeline created\")\n",
    "    else:\n",
    "        print(\"✓ RRF search pipeline already exists\")\n",
    "    \n",
    "    # Get current index statistics\n",
    "    stats = opensearch_client.get_index_stats()\n",
    "    if stats and 'error' not in stats:\n",
    "        print(f\"\\nCurrent Statistics:\")\n",
    "        print(f\"   Documents: {stats.get('document_count', 0)}\")\n",
    "        print(f\"   Size: {stats.get('size_in_bytes', 0):,} bytes\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error with index management: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Data Pipeline - Index Papers from PostgreSQL\n",
    "\n",
    "Transfer papers from PostgreSQL (Week 2) into OpenSearch for search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PIPELINE: PostgreSQL -> OpenSearch\n",
      "==================================================\n",
      "2026-01-29 23:53:05,317 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2026-01-29 23:53:05,322 INFO sqlalchemy.engine.Engine SELECT papers.id AS papers_id, papers.arxiv_id AS papers_arxiv_id, papers.title AS papers_title, papers.authors AS papers_authors, papers.abstract AS papers_abstract, papers.categories AS papers_categories, papers.published_date AS papers_published_date, papers.updated_date AS papers_updated_date, papers.pdf_url AS papers_pdf_url, papers.pdf_content AS papers_pdf_content, papers.sections AS papers_sections, papers.parsing_status AS papers_parsing_status, papers.parsing_error AS papers_parsing_error, papers.created_at AS papers_created_at, papers.updated_at AS papers_updated_at \n",
      "FROM papers\n",
      "2026-01-29 23:53:05,323 INFO sqlalchemy.engine.Engine [generated in 0.00107s] {}\n",
      "Found 5 papers in PostgreSQL\n",
      "\n",
      "Indexing 5 documents into OpenSearch...\n",
      "  ✓ [2508.11121] Tabularis Formatus: Predictive Formatting for Tabl...\n",
      "  ✓ [2601.16210] PyraTok: Language-Aligned Pyramidal Tokenizer for ...\n",
      "  ✓ [2508.11112] Quantization through Piecewise-Affine Regularizati...\n",
      "  ✓ [2508.11110] Diffusion is a code repair operator and generator...\n",
      "  ✓ [2601.16211] Why Can't I Open My Drawer? Mitigating Object-Driv...\n",
      "\n",
      "Indexed 5/5 documents successfully\n",
      "\n",
      "Index Statistics:\n",
      "   Total documents: 5\n",
      "   Index size: 210,166 bytes\n",
      "2026-01-29 23:53:05,655 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# Load papers from PostgreSQL and index into OpenSearch\n",
    "from src.db.factory import make_database\n",
    "from src.models.paper import Paper\n",
    "\n",
    "print(\"DATA PIPELINE: PostgreSQL -> OpenSearch\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "database = make_database()\n",
    "\n",
    "try:\n",
    "    with database.get_session() as session:\n",
    "        # Get all papers from PostgreSQL\n",
    "        papers = session.query(Paper).all()\n",
    "        print(f\"Found {len(papers)} papers in PostgreSQL\")\n",
    "        \n",
    "        if not papers:\n",
    "            print(\"\\nNo papers found! Please run Week 2 notebook first to fetch papers.\")\n",
    "        else:\n",
    "            # Prepare documents for indexing\n",
    "            # Only include fields defined in ARXIV_PAPERS_CHUNKS_MAPPING\n",
    "            docs = []\n",
    "            for paper in papers:\n",
    "                doc = {\n",
    "                    \"arxiv_id\": paper.arxiv_id,\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": paper.authors,\n",
    "                    \"abstract\": paper.abstract,\n",
    "                    \"categories\": paper.categories,\n",
    "                    \"published_date\": paper.published_date.isoformat() if paper.published_date else None,\n",
    "                    \"chunk_text\": paper.abstract,  # Use abstract as chunk text for now\n",
    "                    \"chunk_index\": 0,\n",
    "                    \"chunk_id\": f\"{paper.arxiv_id}_0\",\n",
    "                    \"chunk_word_count\": len(paper.abstract.split()) if paper.abstract else 0,\n",
    "                    \"created_at\": paper.created_at.isoformat() if paper.created_at else None,\n",
    "                    \"updated_at\": paper.updated_at.isoformat() if paper.updated_at else None,\n",
    "                }\n",
    "                docs.append(doc)\n",
    "            \n",
    "            print(f\"\\nIndexing {len(docs)} documents into OpenSearch...\")\n",
    "            \n",
    "            # Index documents one by one (for visibility)\n",
    "            success_count = 0\n",
    "            for doc in docs:\n",
    "                try:\n",
    "                    response = opensearch_client.client.index(\n",
    "                        index=opensearch_client.index_name,\n",
    "                        id=doc[\"chunk_id\"],\n",
    "                        body=doc,\n",
    "                        refresh=True\n",
    "                    )\n",
    "                    if response[\"result\"] in [\"created\", \"updated\"]:\n",
    "                        success_count += 1\n",
    "                        print(f\"  ✓ [{doc['arxiv_id']}] {doc['title'][:50]}...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ [{doc['arxiv_id']}] Error: {e}\")\n",
    "            \n",
    "            print(f\"\\nIndexed {success_count}/{len(docs)} documents successfully\")\n",
    "            \n",
    "            # Verify index stats\n",
    "            stats = opensearch_client.get_index_stats()\n",
    "            print(f\"\\nIndex Statistics:\")\n",
    "            print(f\"   Total documents: {stats.get('document_count', 0)}\")\n",
    "            print(f\"   Index size: {stats.get('size_in_bytes', 0):,} bytes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Simple BM25 Search\n",
    "\n",
    "Let's start with a simple search to demonstrate BM25 scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE BM25 SEARCH\n",
      "========================================\n",
      "Searching for: 'learning'\n",
      "\n",
      "Found 4 total matches\n",
      "\n",
      "1. Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero...\n",
      "   Score: 1.12\n",
      "   arXiv ID: 2601.16211\n",
      "\n",
      "2. PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding ...\n",
      "   Score: 1.22\n",
      "   arXiv ID: 2601.16210\n",
      "\n",
      "3. Tabularis Formatus: Predictive Formatting for Tables...\n",
      "   Score: 0.83\n",
      "   arXiv ID: 2508.11121\n",
      "\n",
      "4. Quantization through Piecewise-Affine Regularization: Optimization and...\n",
      "   Score: 0.92\n",
      "   arXiv ID: 2508.11112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple BM25 Search\n",
    "print(\"SIMPLE BM25 SEARCH\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Change this to any word from your papers\n",
    "search_term = \"learning\"  # Try different terms!\n",
    "\n",
    "print(f\"Searching for: '{search_term}'\\n\")\n",
    "\n",
    "results = opensearch_client.search_papers(\n",
    "    query=search_term,\n",
    "    size=5\n",
    ")\n",
    "\n",
    "if results.get('hits'):\n",
    "    print(f\"Found {results.get('total', 0)} total matches\\n\")\n",
    "    \n",
    "    for i, paper in enumerate(results['hits'], 1):\n",
    "        print(f\"{i}. {paper.get('title', 'Unknown')[:70]}...\")\n",
    "        print(f\"   Score: {paper.get('score', 0):.2f}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\\n\")\n",
    "else:\n",
    "    print(\"No results found. Try searching for:\")\n",
    "    print(\"  • 'neural', 'model', 'algorithm'\")\n",
    "    print(\"  • Use '*' to see all papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 5. Advanced OpenSearch Queries\n",
    "\n",
    "Now let's explore different query types using the OpenSearch Python client directly. This shows the power of BM25 without needing vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### 5.1 Match Query\n",
    "\n",
    "The `match` query is the standard query for full-text search on a single field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH QUERY - Single Field Search\n",
      "========================================\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Match Query - Search in title field\n",
    "print(\"MATCH QUERY - Single Field Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"title\": \"machine learning\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title'][:70]}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### 5.2 Multi-Match Query\n",
    "\n",
    "Search across multiple fields simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-MATCH QUERY - Search Multiple Fields\n",
      "========================================\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-Match Query - Search across multiple fields\n",
    "print(\"MULTI-MATCH QUERY - Search Multiple Fields\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": \"AI Agents\",\n",
    "            \"fields\": [\"title^2\", \"abstract\", \"authors\"],  # ^2 boosts title field\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Title: {hit['_source']['title'][:70]}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\")\n",
    "    authors = hit['_source'].get('authors', [])\n",
    "    if authors:\n",
    "        print(f\"Authors: {', '.join(authors[:2])}...\\n\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### 5.3 Boosting Query\n",
    "\n",
    "Boost certain results while demoting others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOSTING QUERY - Promote/Demote Results\n",
      "========================================\n",
      "Query: Boost 'deep learning', demote 'multimodal' papers\n",
      "\n",
      "Found 4 results\n",
      "\n",
      "Title: PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding ...\n",
      "Score: 0.41\n",
      "Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet ex...\n",
      "\n",
      "Title: Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero...\n",
      "Score: 0.37\n",
      "Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and ...\n",
      "\n",
      "Title: Quantization through Piecewise-Affine Regularization: Optimization and...\n",
      "Score: 0.31\n",
      "Abstract: Optimization problems over discrete or quantized variables are very challenging in general due to th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Boosting Query - Promote and demote results\n",
    "print(\"BOOSTING QUERY - Promote/Demote Results\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"boosting\": {\n",
    "            \"positive\": {\n",
    "                \"match\": {\n",
    "                    \"abstract\": \"deep learning\"\n",
    "                }\n",
    "            },\n",
    "            \"negative\": {\n",
    "                \"match\": {\n",
    "                    \"abstract\": \"multimodal\"\n",
    "                }\n",
    "            },\n",
    "            \"negative_boost\": 0.1  # Reduce score of negative matches\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Query: Boost 'deep learning', demote 'multimodal' papers\\n\")\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    abstract_snippet = hit['_source'].get('abstract', '')[:100]\n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\")\n",
    "    print(f\"Abstract: {abstract_snippet}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 5.4 Filter Query\n",
    "\n",
    "Filter results by specific criteria (doesn't affect scoring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILTER QUERY - Category Filtering\n",
      "========================================\n",
      "Found 1 results\n",
      "\n",
      "Title: Tabularis Formatus: Predictive Formatting for Tables...\n",
      "Categories: cs.DB, cs.AI, cs.SE\n",
      "Score: 1.34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Query - Filter by categories\n",
    "print(\"FILTER QUERY - Category Filtering\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"abstract\": \"neural\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\n",
    "                    \"terms\": {\n",
    "                        \"categories\": [\"cs.AI\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    categories = ', '.join(hit['_source'].get('categories', []))\n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"Categories: {categories}\")\n",
    "    print(f\"Score: {hit['_score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 5.5 Sorting Query\n",
    "\n",
    "Sort results by different criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SORTING QUERY - Latest Papers First\n",
      "========================================\n",
      "Query: All papers sorted by publication date (newest first)\n",
      "\n",
      "Date: 2026-01-22 | Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero...\n",
      "Date: 2026-01-22 | PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding ...\n",
      "Date: 2025-08-14 | Tabularis Formatus: Predictive Formatting for Tables...\n",
      "Date: 2025-08-14 | Quantization through Piecewise-Affine Regularization: Optimization and...\n",
      "Date: 2025-08-14 | Diffusion is a code repair operator and generator...\n"
     ]
    }
   ],
   "source": [
    "# Sorting Query - Sort by publication date\n",
    "print(\"SORTING QUERY - Latest Papers First\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match_all\": {}  # Get all papers\n",
    "    },\n",
    "    \"sort\": [\n",
    "        {\n",
    "            \"published_date\": {\n",
    "                \"order\": \"desc\"  # Latest first\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"size\": 5\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Query: All papers sorted by publication date (newest first)\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    pub_date = str(hit['_source'].get('published_date', 'N/A'))[:10]\n",
    "    print(f\"Date: {pub_date} | {title}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 5.6 Combined Query\n",
    "\n",
    "Combine multiple query types for complex searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED QUERY - Complex Search\n",
      "========================================\n",
      "Complex Query:\n",
      "  • Must contain 'transformer' (title boosted 3x)\n",
      "  • Filter: published after 2024-01-01\n",
      "  • Prefer: cs.AI category\n",
      "  • Sort: by relevance, then date\n",
      "\n",
      "Found 0 results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Query - Complex search with multiple criteria\n",
    "print(\"COMBINED QUERY - Complex Search\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": \"transformer\",\n",
    "                        \"fields\": [\"title^3\", \"abstract\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [\n",
    "                {\n",
    "                    \"range\": {\n",
    "                        \"published_date\": {\n",
    "                            \"gte\": \"2024-01-01\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"categories\": \"cs.AI\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"sort\": [\n",
    "        \"_score\",\n",
    "        {\"published_date\": {\"order\": \"desc\"}}\n",
    "    ],\n",
    "    \"size\": 3\n",
    "}\n",
    "\n",
    "response = opensearch_client.client.search(\n",
    "    index=opensearch_client.index_name,\n",
    "    body=query\n",
    ")\n",
    "\n",
    "print(f\"Complex Query:\")\n",
    "print(f\"  • Must contain 'transformer' (title boosted 3x)\")\n",
    "print(f\"  • Filter: published after 2024-01-01\")\n",
    "print(f\"  • Prefer: cs.AI category\")\n",
    "print(f\"  • Sort: by relevance, then date\\n\")\n",
    "\n",
    "print(f\"Found {response['hits']['total']['value']} results\\n\")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    title = hit['_source']['title'][:70]\n",
    "    pub_date = str(hit['_source'].get('published_date', 'N/A'))[:10]\n",
    "    score = hit['_score']\n",
    "    categories = ', '.join(hit['_source'].get('categories', [])[:2])\n",
    "    \n",
    "    print(f\"Title: {title}...\")\n",
    "    print(f\"  Date: {pub_date} | Score: {score:.2f}\")\n",
    "    print(f\"  Categories: {categories}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 6. Test Two-Letter Queries (AI, ML, NN, CV)\n",
    "\n",
    "Specifically testing short queries that are common in academic search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO-LETTER QUERY TEST\n",
      "========================================\n",
      "\n",
      "'AI' -> 0 results\n",
      "\n",
      "'ML' -> 0 results\n",
      "\n",
      "'NN' -> 0 results\n",
      "\n",
      "'CV' -> 0 results\n",
      "\n",
      "'NLP' -> 0 results\n",
      "\n",
      "'LLM' -> 0 results\n"
     ]
    }
   ],
   "source": [
    "# Test Two-Letter Queries\n",
    "print(\"TWO-LETTER QUERY TEST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "two_letter_queries = [\"AI\", \"ML\", \"NN\", \"CV\", \"NLP\", \"LLM\"]\n",
    "\n",
    "for q in two_letter_queries:\n",
    "    results = opensearch_client.search_papers(query=q, size=2)\n",
    "    total = results.get('total', 0)\n",
    "    \n",
    "    print(f\"\\n'{q}' -> {total} results\")\n",
    "    \n",
    "    for hit in results.get('hits', []):\n",
    "        title = hit.get('title', 'N/A')[:60]\n",
    "        score = hit.get('score', 0)\n",
    "        print(f\"  [{score:.2f}] {title}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 7. Test Search API Endpoints\n",
    "\n",
    "Verify the FastAPI search endpoints are working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH API ENDPOINT TESTS\n",
      "==================================================\n",
      "\n",
      "--- Test 1: Health Check ---\n",
      "✓ Status: ok\n",
      "  database: healthy - Connected successfully\n",
      "  opensearch: healthy - Index 'arxiv-papers-chunks' with 5 documents\n",
      "\n",
      "--- Test 2: GET /search ---\n",
      "✓ Found 1 results for 'neural network'\n",
      "  [6.42] Tabularis Formatus: Predictive Formatting for Tables...\n",
      "\n",
      "--- Test 3: POST /search ---\n",
      "✓ Found 4 results\n",
      "  Search mode: bm25\n",
      "  [4.44] PyraTok: Language-Aligned Pyramidal Tokenizer for Video Unde...\n",
      "  [1.12] Why Can't I Open My Drawer? Mitigating Object-Driven Shortcu...\n",
      "  [0.92] Quantization through Piecewise-Affine Regularization: Optimi...\n",
      "\n",
      "==================================================\n",
      "API endpoint tests complete!\n",
      "\n",
      "Swagger UI: http://localhost:8000/docs\n"
     ]
    }
   ],
   "source": [
    "# Test Search API Endpoints\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_BASE = \"http://localhost:8000/api/v1\"\n",
    "\n",
    "print(\"SEARCH API ENDPOINT TESTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Health check\n",
    "print(\"\\n--- Test 1: Health Check ---\")\n",
    "try:\n",
    "    response = requests.get(f\"{API_BASE}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        health = response.json()\n",
    "        print(f\"✓ Status: {health.get('status', 'unknown')}\")\n",
    "        for service, status in health.get('services', {}).items():\n",
    "            print(f\"  {service}: {status.get('status', 'unknown')} - {status.get('message', '')}\")\n",
    "    else:\n",
    "        print(f\"✗ Health check returned: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Health check error: {e}\")\n",
    "    print(\"  Make sure the API is running with the updated main.py\")\n",
    "\n",
    "# Test 2: GET /search\n",
    "print(\"\\n--- Test 2: GET /search ---\")\n",
    "try:\n",
    "    response = requests.get(\n",
    "        f\"{API_BASE}/search\",\n",
    "        params={\"q\": \"neural network\", \"size\": 3},\n",
    "        timeout=5\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"✓ Found {data.get('total', 0)} results for 'neural network'\")\n",
    "        for hit in data.get('hits', [])[:3]:\n",
    "            print(f\"  [{hit.get('score', 0):.2f}] {hit.get('title', 'N/A')[:60]}...\")\n",
    "    else:\n",
    "        print(f\"✗ Search returned: {response.status_code}\")\n",
    "        print(f\"  Response: {response.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Search error: {e}\")\n",
    "\n",
    "# Test 3: POST /search\n",
    "print(\"\\n--- Test 3: POST /search ---\")\n",
    "try:\n",
    "    search_body = {\n",
    "        \"query\": \"deep learning transformer\",\n",
    "        \"size\": 3,\n",
    "        \"categories\": [\"cs.AI\"],\n",
    "        \"latest_papers\": False\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f\"{API_BASE}/search\",\n",
    "        json=search_body,\n",
    "        timeout=5\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"✓ Found {data.get('total', 0)} results\")\n",
    "        print(f\"  Search mode: {data.get('search_mode', 'N/A')}\")\n",
    "        for hit in data.get('hits', [])[:3]:\n",
    "            print(f\"  [{hit.get('score', 0):.2f}] {hit.get('title', 'N/A')[:60]}...\")\n",
    "    else:\n",
    "        print(f\"✗ Search returned: {response.status_code}\")\n",
    "        print(f\"  Response: {response.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Search error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"API endpoint tests complete!\")\n",
    "print(f\"\\nSwagger UI: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "**BM25 Search is Powerful!** Without any vector embeddings, we can:\n",
    "\n",
    "1. **Simple Search**: Basic keyword search with relevance scoring\n",
    "2. **Match Queries**: Search specific fields\n",
    "3. **Multi-Match**: Search across multiple fields with boosting\n",
    "4. **Boosting**: Promote or demote certain results\n",
    "5. **Filtering**: Apply filters without affecting scores\n",
    "6. **Sorting**: Order results by date, score, or other fields\n",
    "7. **Complex Queries**: Combine all techniques for sophisticated searches\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **BM25 works great** for many search use cases\n",
    "- **No vectors needed** for effective full-text search\n",
    "- **Simple and fast** compared to embedding-based approaches\n",
    "- **Filters and sorting** make searches precise and relevant\n",
    "- **Field boosting** helps prioritize important content\n",
    "\n",
    "### When to Use BM25 vs Vectors\n",
    "\n",
    "**Use BM25 when:**\n",
    "- Searching for specific keywords or phrases\n",
    "- Need fast, simple implementation\n",
    "- Have good text fields with clear terminology\n",
    "- Want explainable search results\n",
    "\n",
    "**Consider vectors when:**\n",
    "- Need semantic similarity (concepts, not keywords)\n",
    "- Dealing with synonyms and paraphrasing\n",
    "- Cross-language search requirements\n",
    "- Very short queries or documents\n",
    "\n",
    "Remember: **You can also combine both** (hybrid search) for best results!\n",
    "We will see this in Week 4+."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
